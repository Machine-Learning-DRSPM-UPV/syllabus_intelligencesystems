{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Decision Trees\n",
    "\n",
    "This section introduces you to two types of supervised learning algorithms in detail. The first algorithm will help you classify data points using decision trees, while the other algorithm will help you classify data points using random forests. Furthermore, you'll learn how to calculate the precision, recall, and F1 score of models, both manually and automatically. By the end, you will be able to analyze the metrics that are used for evaluating the utility of a data model and classify data points based on decision trees and random forest algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Previouly, we learned the difference between regression and classification problems, and we saw how to train some of the most famous algorithms. Here, we will look at another type of algorithm: tree-based models.\n",
    "\n",
    "Tree-based models are very popular as they can model complex non-linear patterns and they are relatively easy to interpret. We will introduce you to decision trees and the random forest algorithms, which are some of the most widely used tree-based models in the industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "A decision tree has leaves, branches, and nodes. Nodes are where a decision is made. A decision tree consists of rules that we use to formulate a decision (or prediction) on the prediction of a data point.\n",
    "\n",
    "Every node of the decision tree represents a feature, while every edge coming out of an internal node represents a possible value or a possible interval of values of the tree. Each leaf of the tree represents a label value of the tree.\n",
    "\n",
    "This may sound complicated, but let's look at an application of this.\n",
    "\n",
    "Suppose we have a dataset with the following features and the response variable is determining whether a person is creditworthy or not:\n",
    "\n",
    "![Figure 4.1](img/fig4_01.jpg)\n",
    "\n",
    "A decision tree, remember, is just a group of rules. Looking at the dataset in Figure 4.1, we can come up with the following rules:\n",
    "\n",
    "  * All people with house loans are determined as creditworthy.\n",
    "  * If debtors are employed and studying, then loans are creditworthy.\n",
    "  * People with income above 75,000 a year are creditworthy.\n",
    "  * At or below 75,000 a year, people with car loans and who are employed are creditworthy.\n",
    "  \n",
    "Following the order of the rules we just defined, we can build a tree, as shown in Figure 4.2 and describe one possible credit scoring method:\n",
    "\n",
    "![Figure 4.2](img/fig4_02.jpg)\n",
    "\n",
    "First, we determine the loan type. House loans are automatically creditworthy according to the first rule. Study loans are described by the second rule, resulting in a subtree containing another decision on employment. Since we have covered both house and study loans, there are only car loans left. The third rule describes an income decision, while the fourth rule describes a decision on employment.\n",
    "\n",
    "Whenever we must score a new debtor to determine whether they are creditworthy, we have to go through the decision tree from top to bottom and observe the true or false value at the bottom.\n",
    "\n",
    "Obviously, a model based on seven data points is highly inaccurate because we can't generalize rules that simply do not match reality. Therefore, rules are often determined based on large amounts of data.\n",
    "\n",
    "This is not the only way that we can create a decision tree. We can build decision trees based on other sequences of rules, too. Let's extract some other rules from the dataset in Figure 4.1.\n",
    "\n",
    "**Observation 1**: Notice that individual salaries that are greater than 75,000 are all creditworthy.\n",
    "\n",
    "**Rule 1**: `Income > 75,000 => CreditWorthy` is true.\n",
    "\n",
    "Rule 1 classifies four out of seven data points (IDs C, E, F, G); we need more rules for the remaining three data points.\n",
    "\n",
    "**Observation 2**: Out of the remaining three data points, two are not employed. One is employed (ID D) and is creditworthy. With this, we can claim the following rule:\n",
    "\n",
    "**Rule 2**: Assuming `Income <= 75,000`, the following holds true: `Employed == true => CreditWorthy`.\n",
    "\n",
    "Note that with this second rule, we can also classify the remaining two data points (IDs A and B) as not creditworthy. With just two rules, we accurately classified all the observations from this dataset:\n",
    "\n",
    "![Figure 4.3](img/fig4_03.jpg)\n",
    "\n",
    "The second decision tree is less complex. At the same time, we cannot overlook the fact that the model says, employed people with a lower income are less likely to pay back their loans. Unfortunately, there is not enough training data available (there are only seven observations in this example), which makes it likely that we'll end up with false conclusions.\n",
    "\n",
    "Overfitting is a frequent problem in decision trees when making a decision based on a few data points. This decision is rarely representative.\n",
    "\n",
    "Since we can build decision trees in any possible order, it makes sense to define an efficient way of constructing a decision tree. Therefore, we will now explore a measure for ordering the features in the decision process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "In information theory, entropy measures how randomly distributed the possible values of an attribute are. The higher the degree of randomness is, the higher the entropy of the attribute.\n",
    "\n",
    "Entropy is the highest possibility of an event. If we know beforehand what the outcome will be, then the event has no randomness. So, entropy is **zero**.\n",
    "\n",
    "We use entropy to order the splitting of nodes in the decision tree. Taking the previous example, which rule should we start with? Should it be `Income <= 75000` or `is employed`? We need to use a metric that can tell us that one specific split is better than the other. A good split can be defined by the fact it clearly split the data into two homogenous groups. One of these metrics is information gain, and it is based on entropy.\n",
    "\n",
    "Here is the formula for calculating entropy:\n",
    "\n",
    "$$\n",
    "H\\left(distribution\\right) =  \\sum_{i=0}^n -p_i \\log{}_i p_i\n",
    "$$\n",
    "\n",
    "$p_i$ represents the probability of one of the possible values of the target variable occurring. So, if this column has n different unique values, then we will have the probability for each of them ($[p_1, p_2, \\dots, p_n]$) and apply the formula.\n",
    "\n",
    "To manually calculate the entropy of a distribution in Python, we can use the `np.log2` and `np.dot()` methods from the NumPy library. There is no function in `numpy` to automatically calculate entropy.\n",
    "\n",
    "Have a look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.754887502163468"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "proba = list(range(1, 4))\n",
    "\n",
    "def entropy(probabilities):\n",
    "    minus_probabilities = [-x for x in probabilities]\n",
    "    log_probabilities = [x for x in map(np.log2, probabilities)]\n",
    "    entropy_value = np.dot(minus_probabilities, log_probabilities)\n",
    "    return entropy_value\n",
    "\n",
    "entropy(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities are given as a NumPy array or a regular list on line 2: $p_i$.\n",
    "\n",
    "We need to create a vector of the negated values of the distribution in line 3: - $p_i$.\n",
    "\n",
    "In line 4, we must take the base two logarithms of each value in the distribution list: $\\log{}_i p_i$.\n",
    "\n",
    "Finally, we calculate the sum with the scalar product, also known as the dot product of the two vectors:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^n -p_i \\log_i p_i\n",
    "$$\n",
    "\n",
    "In the exercise 4.01, we will be calculating entropy on a small sample dataset.\n",
    "\n",
    "**Go to Exercise 4.01**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "When we partition the data points in a dataset according to the values of an attribute, we reduce the entropy of the system.\n",
    "\n",
    "To describe information gain, we can calculate the distribution of the labels. Initially, in Figure 4.1, we had five creditworthy and two not creditworthy individuals in our dataset. The entropy belonging to the initial distribution is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.863120568566631"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_label = entropy([5/7, 2/7])\n",
    "H_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we partition the dataset based on whether the loan amount is greater than 15,000 or not:\n",
    "\n",
    "  * In group 1, we get one data point belonging to the 15,000 loan amount. This data point is not creditworthy.\n",
    "  * In group 2, we have five creditworthy individuals and one non-creditworthy individual.\n",
    "  \n",
    "The entropy of the labels in each group is as follows.\n",
    "\n",
    "For group 1, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_group1 = entropy([1])\n",
    "H_group1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For group 2, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6500224216483541"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_group2 = entropy([5/6, 1/6])\n",
    "H_group2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the information gain, let's calculate the weighted average of the group entropies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5571620756985892"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_average = H_group1 * 1/7 + H_group2 * 6/7\n",
    "w_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to find the information gain, we need to calculate the difference between the original entropy (`H_label`) and the one we just calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3059584928680418"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain = H_label - w_average\n",
    "information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By splitting the data with this rule, we gain a little bit of information.\n",
    "\n",
    "When creating the decision tree, on each node, our job is to partition the dataset using a rule that maximizes the information gain.\n",
    "\n",
    "We could also use Gini Impurity instead of entropy-based information gain to construct the best rules for splitting decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity\n",
    "\n",
    "Instead of entropy, there is another widely used metric that can be used to measure the randomness of a distribution: Gini Impurity.\n",
    "\n",
    "Gini Impurity is defined as follows:\n",
    "\n",
    "$$\n",
    "Gini(distribution) = 1 - \\sum_{i=0}^n p_i^2\n",
    "$$\n",
    "\n",
    "$p_i$ here represents the probability of one of the possible values of the target variable occurring.\n",
    "\n",
    "Entropy may be a bit slower to calculate because of the usage of the logarithm. Gini Impurity, on the other hand, is less precise when it comes to measuring randomness.\n",
    "\n",
    "With this, we have learned that we can optimize a decision tree by splitting the data based on information gain or Gini Impurity. Unfortunately, these metrics are only available for discrete values. What if the label is defined on a continuous interval such as a price range or salary range?\n",
    "\n",
    "We have to use other metrics. You can technically understand the idea behind creating a decision tree based on a continuous label, which was about regression. One metric we can reuse in this chapter is the mean squared error. Instead of Gini Impurity or information gain, we have to minimize the mean squared error to optimize the decision tree. As this is a beginner's course, we will omit this metric.\n",
    "\n",
    "In the next section, we will discuss the exit condition for a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Condition\n",
    "\n",
    "We can continuously split the data points according to more and more specific rules until each leaf of the decision tree has an entropy of zero. The question is whether this end state is desirable.\n",
    "\n",
    "Often, this is not what we expect, because we risk overfitting the model. When our rules for the model are too specific and too nitpicky, and the sample size that the decision was made on is too small, we risk making a false conclusion, thus recognizing a pattern in the dataset that simply does not exist in real life.\n",
    "\n",
    "For instance, if we spin a roulette wheel three times and we get 12, 25, and 12, this concludes that every odd spin resulting in the value 12 is not a sensible strategy. By assuming that every odd spin equals 12, we find a rule that is exclusively due to random noise.\n",
    "\n",
    "Therefore, posing a restriction on the minimum size of the dataset that we can still split is an exit condition that works well in practice. For instance, if you stop splitting as soon as you have a dataset that's lower than 50, 100, 200, or 500 in size, you avoid drawing conclusions on random noise, and so you minimize the risk of overfitting the model.\n",
    "\n",
    "Another popular exit condition is the maximum restriction on the depth of the tree. Once we reach a fixed tree depth, we classify the data points in the leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Decision Tree Classifiers Using scikit-learn\n",
    "\n",
    "We have already learned how to load data from a `.csv` file, how to apply preprocessing to data, and how to split data into training and testing datasets. If you need to refresh yourself on this knowledge, you can go back, where you can go through this process in the context of regression and classification.\n",
    "\n",
    "Now, we will assume that a set of training features, training labels, testing features, and testing labels have been given as a return value of the scikit-learn `train-test-split` call:\n",
    "\n",
    "```Python\n",
    "from sklearn import model_selection\n",
    "features_train, features_test, \\\n",
    "label_train, label_test = \\\n",
    "model_selection.train_test_split(features, label, test_size=0.1, \\\n",
    "                                 random_state=8)\n",
    "```\n",
    "\n",
    "In the preceding code snippet, we used `train_test_split` to split the dataset (features and labels) into training and testing sets. The testing set represents 10% of the observation (`test_size=0.1`). The `random_state` parameter is used to get reproducible results.\n",
    "\n",
    "We will not focus on how we got these data points because this process is exactly the same as in the case of regression and classification.\n",
    "\n",
    "It's time to import and use the decision tree classifier of scikit-learn:\n",
    "\n",
    "```Python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(max_depth=6)\n",
    "decision_tree.fit(features_train, label_train)\n",
    "```\n",
    "\n",
    "We set one optional parameter in `DecisionTreeClassifier`, that is, `max_depth`, to limit the depth of the decision tree.\n",
    "\n",
    "  > You can read the official documentation for the full list of parameters: [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "  \n",
    "Some of the more important parameters are as follows:\n",
    "\n",
    "  * `criterion`: Gini stands for Gini Impurity, while entropy stands for information gain. This will define which measure will be used to assess the quality of a split at each node.\n",
    "  * `max_depth`: This is the parameter that defines the maximum depth of the tree.\n",
    "  * `min_samples_split`: This is the minimum number of samples needed to split an internal node.\n",
    "  \n",
    "You can also experiment with all the other parameters that were enumerated in the documentation. We will omit them in this section.\n",
    "\n",
    "Once the model has been built, we can use the decision tree classifier to predict data:\n",
    "\n",
    "```Python\n",
    "decision_tree.predict(features_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics for Classifiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the training and testing data, the decision tree model has a `score` method to evaluate how well testing data is classified by the model (also known as the accuracy score). We learned how to use the `score` method in the previous two chapters:\n",
    "\n",
    "```Python\n",
    "decision_tree.score(features_test, label_test)\n",
    "```\n",
    "\n",
    "The return value of the `score` method is a number that's less than or equal to 1. The closer we get to 1, the better our model is.\n",
    "\n",
    "Now, we will learn about another way to evaluate the model.\n",
    "\n",
    "Suppose we have one test feature and one test label:\n",
    "\n",
    "```Python\n",
    "predicted_label = decision_tree.predict(features_test)\n",
    "```\n",
    "\n",
    "Let's use the previous creditworthy example and assume we trained a decision tree and now have its predictions:\n",
    "\n",
    "![Figure 4.8](img/fig4_08.jpg)\n",
    "\n",
    "\n",
    "Our model, in general, made good predictions but had few errors. It incorrectly predicted the results for IDs A, D, and E. Its accuracy score will be 4/7 = 0.57.\n",
    "\n",
    "We will use the following definitions to define some metrics that will help you evaluate how good your classifier is:\n",
    "\n",
    "  * **True positive (or TP)**: All the observations where the true label (the `Creditworthy` column, in our example) and the corresponding predictions both have the value `Yes`. In our example, IDs $C$, $F$, and $G$ will fall under this category.\n",
    "  * **True negative (or TN)**: All the observations where the true label and the corresponding predictions both have the value `No`. Only ID $B$ will be classified as true negative.\n",
    "  * **False positive (or FP)**: All the observations where the prediction is `Yes` but the true label is actually `No`. This will be the case for ID $A$.\n",
    "  * **False negative (or FN)**: All the observations where the prediction is `No` but the true label is actually `Yes`, such as for IDs $D$ and $E$.\n",
    "  \n",
    "Using the preceding four definitions, we can define four metrics that describe how well our model predicts the target variable. The `#( X )` symbol denotes the number of values in X. Using technical terms, `#( X )` denotes the cardinality of `X`:\n",
    "\n",
    "**Definition (Accuracy)**: #( True Positives ) + #( True Negatives ) / #( Dataset )\n",
    "\n",
    "Accuracy is a metric that's used for determining how many times the classifier gives us the correct answer. This is the first metric we used to evaluate the score of a classifier.\n",
    "\n",
    "In our previous example (Figure 4.8), the accuracy score will be TP + TN / total = (3 + 1) / 7 = 4/7.\n",
    "\n",
    "We can use the function provided by scikit-learn to calculate the accuracy of a model:\n",
    "\n",
    "```Python\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(label_test, predicted_label)\n",
    "```\n",
    "\n",
    "**Definition (Precision)**: #TruePositives / (#TruePositives + #FalsePositives)\n",
    "\n",
    "Precision centers around values that our classifier found to be positive. Some of these results are true positive, while others are false positive. High precision means that the number of false positive results is very low compared to the true positive results. This means that a precise classifier rarely makes a mistake when finding a positive result.\n",
    "\n",
    "**Definition (Recall)**: #True Positives / (#True Positives + #False Negatives)\n",
    "\n",
    "Recall centers around values that are positive among the test data. Some of these results are found by the classifier. These are the true positive values. Those positive values that are not found by the classifier are false negatives. A classifier with a high recall value finds most of the positive values.\n",
    "\n",
    "Using our previous example (Figure 4.8), we will get the following measures:\n",
    "\n",
    "  * Precision = TP / (TP + FP) = 4 / (4 + 1) = 4/6 = 0.8\n",
    "  * Recall = TP / (TP + FN) = 4 / (4 + 2) = 4/6 = 0.6667\n",
    "  \n",
    "With these two measures, we can easily see where our model is performing better or worse. In this example, we know it tends to misclassify false negative cases. These measures are more granular than the accuracy score, which only gives you an overall score.\n",
    "\n",
    "The $F_1$ score is a metric that combines precision and recall scores. Its value ranges between $0$ and $1$. If the $F_1$ score equals $1$, it means the model is perfectly predicting the right outcomes. On the other hand, an $F_1$ score of $0$ means the model cannot predict the target variable accurately. The advantage of the $F_1$ score is that it considers both false positives and false negatives.\n",
    "\n",
    "The formula for calculating the $F_1$ score is as follows:\n",
    "\n",
    "$$\n",
    "f_1 = 2 * \\frac{precision * recall}{precision + recall}\n",
    "$$\n",
    "\n",
    "As a final note, the scikit-learn package also provides a handy function that can show all these measures in one go: `classification_report()`. A classification report is useful to check the quality of our predictions:\n",
    "\n",
    "```Python\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_test, predicted_label))\n",
    "```\n",
    "\n",
    "**Go to Exercise 4.02**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Performance of Classifiers with scikit-learn\n",
    "\n",
    "The scikit-learn package provides some functions for automatically calculating the precision, recall, and F1 score for you. You will need to import them first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "label_test = np.array([True, True, False, True, True])\n",
    "predicted_label = np.array([True, False, False, False, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the precision score, you will need to get the predictions from your model, as shown in the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(label_test, predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the recall_score can be done like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(label_test, predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the `f1_score` can be done like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(label_test, predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Confusion Matrix\n",
    "\n",
    "Previously, we learned how to use some calculated metrics to assess the performance of a classifier. There is another very interesting tool that can help you evaluate the performance of a multi-class classification model: the confusion matrix.\n",
    "\n",
    "A confusion matrix is a square matrix where the number of rows and columns equals the number of distinct label values (or classes). In the columns of the matrix, we place each test label value. In the rows of the matrix, we place each predicted label value.\n",
    "\n",
    "A confusion matrix looks like this:\n",
    "\n",
    "![Figure 4.10](img/fig4_10.jpg)\n",
    "\n",
    "In the preceding example, the first row of the confusion matrix is showing us that the model is doing the following:\n",
    "\n",
    "  * Correctly predicting class `A` $88$ times\n",
    "  * Predicting class `A` when the true value is `B` $3$ times\n",
    "  * Predicting class `A` when the true value is `C` 2 times\n",
    "  \n",
    "We can also see the scenario where the model is making a lot of mistakes when it is predicting C while the true value is A (16 times). A confusion matrix is a powerful tool to quickly and easily spot which classes your model is performing well or badly for.\n",
    "\n",
    "The scikit-learn package provides a function to calculate and display a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [3, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(label_test, predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "If you think about the name random forest classifier, it can be explained as follows:\n",
    "\n",
    "  * A forest consists of multiple trees.\n",
    "  * These trees can be used for classification.\n",
    "  * Since the only tree we have used so far for classification is a decision tree, it makes sense that the random forest is a forest of decision trees.\n",
    "  * The random nature of the trees means that our decision trees are constructed in a randomized manner.\n",
    "  \n",
    "Therefore, we will base our decision tree construction on information gain or Gini Impurity.\n",
    "\n",
    "Once you understand these basic concepts, you essentially know what a random forest classifier is all about. The more trees you have in the forest, the more accurate prediction is going to be. When performing prediction, each tree performs classification. We collect the results, and the class that gets the most votes wins.\n",
    "\n",
    "Random forests can be used for regression as well as for classification. When using random forests for regression, instead of counting the most votes for a class, we take the average of the arithmetic mean (average) of the prediction results and return it. Random forests are not as ideal for regression as they are for classification, though, because the models that are used to predict values are often out of control, and often return a wide range of values. The average of these values is often not too meaningful. Managing the noise in a regression exercise is harder than in classification.\n",
    "\n",
    "Random forests are often better than one simple decision tree because they provide redundancy. They treat outlier values better and have a lower probability of overfitting the model. Decision trees seem to behave great as long as you are using them on the data that was used when creating the model. Once you use them to predict new data, random forests lose their edge. Random forests are widely used for classification problems, whether it be customer segmentation for banks or e-commerce, classifying images, or medicine. If you own an Xbox with Kinect, your Kinect device contains a random forest classifier to detect your body.\n",
    "\n",
    "Random forest is an ensemble algorithm. The idea behind ensemble learning is that we take an aggregated view of a decision of multiple agents that potentially have different weaknesses. Due to the aggregated vote, these weaknesses cancel out, and the majority vote likely represents the correct result.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification Using scikit-learn\n",
    "\n",
    "As you may have guessed, the scikit-learn package provides an implementation of the `RandomForest` classifier with the RandomForestClassifier class. This class provides the exact same methods as all the scikit-learn models you have seen so far – you need to instantiate a model, then fit it with the training set with `.fit()`, and finally make predictions with `.predict()`:\n",
    "\n",
    "```Python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "random_forest_classifier.fit(features_train, label_train)\n",
    "label_predicted = random_forest_classifier.predict(features_test)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Parameterization of the Random Forest Classifier\n",
    "\n",
    "We will be considering a subset of the possible parameters, based on what we already know, which is based on the description of constructing random forests:\n",
    "\n",
    "  * `n_estimators`: The number of trees in the random forest. The default value is 10.\n",
    "  * `criterion`: Use Gini or entropy to determine whether you use Gini Impurity or information gain using the entropy in each tree. This will be used to find the best split at each node.\n",
    "  * `max_features`: The maximum number of features considered in any tree of the forest. Possible values include an integer. You can also add some strings such as `sqrt` for the square root of the number of features.\n",
    "  * `max_depth`: The maximum depth of each tree.\n",
    "  * `min_samples_split`: The minimum number of samples in the dataset in a given node to perform a split. This may also reduce the tree's size.\n",
    "  * `bootstrap`: A Boolean that indicates whether to use bootstrapping on data points when constructing trees.\n",
    "  \n",
    "### Feature Importance\n",
    "\n",
    "A random forest classifier gives you information on how important each feature in the data classification process is. Remember, we used a lot of randomly constructed decision trees to classify data points. We can measure how accurately these data points behave, and we can also see which features are vital when it comes to decision-making.\n",
    "\n",
    "We can retrieve the array of feature importance scores with the following query:\n",
    "\n",
    "```Python\n",
    "random_forest_classifier.feature_importances_\n",
    "```\n",
    "\n",
    "In this six-feature classifier, the fourth and sixth features are clearly a lot more important than any other features. The third feature has a very low importance score.\n",
    "\n",
    "Feature importance scores come in handy when we have a lot of features and we want to reduce the feature size to avoid the classifier getting lost in the details. When we have a lot of features, we risk overfitting the model. Therefore, reducing the number of features by dropping the least significant ones is often helpful.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "Earlier, we learned how to use different metrics to assess the performance of a classifier, such as the accuracy, precision, recall, or the F1 score on a training and testing set. The objective is to have a high score on both sets that are very close to each other. In that case, your model is performant and not prone to overfitting.\n",
    "\n",
    "The test set is used as a proxy to evaluate whether your model can generalize well to unseen data or whether it learns patterns that are only relevant to the training set.\n",
    "\n",
    "But in the case of having quite a few hyperparameters to tune (such as for `RandomForest`), you will have to train a lot of different models and test them on your testing set. This kind of defeats the purpose of the testing set. Think of the testing set as the final exam that will define whether you pass a subject or not. You will not be allowed to pass and repass it over and over.\n",
    "\n",
    "One solution for avoiding using the testing set too much is creating a validation set. You will train your model on the training set and use the validation set to assess its score according to different combinations of hyperparameters. Once you find your best model, you will use the testing set to make sure it doesn't overfit too much. This is, in general, the suggested approach for any data science project.\n",
    "\n",
    "The drawback of this approach is that you are reducing the number of observations for the training set. If you have a dataset with millions of rows, it is not a problem. But for a small dataset, this can be problematic. This is where cross-validation comes in.\n",
    "\n",
    "The following Figure 4.12, shows that this is a technique where you create multiple splits of the training data. For each split, the training data is separated into folds (five, in this example) and one of the folds will be used as the validation set while the others will be used for training.\n",
    "\n",
    "For instance, for the top split, fold 5 will be used for validation and the four other folds (1 to 4) will be used to train the model. You will follow the same process for each split. After going through each split, you will have used the entire training data and the final performance score will be the average of all the models that were trained on each split:\n",
    "\n",
    "![Figure 4.12](img/fig4_12.jpg)\n",
    "\n",
    "With scikit-learn, you can easily perform cross-validation, as shown in the following code snippet:\n",
    "\n",
    "```Python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "\n",
    "cross_val_score(random_forest_classifier, features_train, \\\n",
    "                label_train, cv=5, scoring='accuracy')\n",
    "```\n",
    "\n",
    "`cross_val_score` takes two parameters:\n",
    "\n",
    "  * `cv`: Specifies the number of splits.\n",
    "  * `scoring`: Defines which performance metrics you want to use. You can find the list of possible values [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).\n",
    "  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees\n",
    "\n",
    "Extremely randomized trees increase the randomization inside random forests by randomizing the splitting rules on top of the already randomized factors in random forests.\n",
    "\n",
    "Parameterization is like the random forest classifier. You can see the full list of parameters [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html).\n",
    "\n",
    "The Python implementation is as follows:\n",
    "\n",
    "```Python\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "extra_trees_classifier = \\\n",
    "ExtraTreesClassifier(n_estimators=100, \\\n",
    "                     max_depth=6)\n",
    "extra_trees_classifier.fit(features_train, label_train)\n",
    "labels_predicted = extra_trees_classifier.predict(features_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('TheAppliedAIWorkshop')",
   "language": "python",
   "name": "python38564bittheappliedaiworkshopbbf3bf7f8981460693d2b1f0fcd6289d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
