{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Classification\n",
    "\n",
    "This section introduces you to classification. You will implement various techniques, such as k-nearest neighbors and SVMs. You will use the Euclidean and Manhattan distances to work with k-nearest neighbors. You will apply these concepts to solve intriguing problems such as predicting whether a credit card applicant has a risk of defaulting and determining whether an employee would stay with a company for more than two years. By the end, you will be confident enough to work with any data using classification and come to a certain conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "previously, you were introduced to regression models and learned how to fit a linear regression model with single or multiple variables, as well as with a higher-degree polynomial.\n",
    "\n",
    "Unlike regression models, which focus on learning how to predict continuous numerical values (which can have an infinite number of values), classification is all about splitting data into separate groups, also called classes.\n",
    "\n",
    "For instance, a model can be trained to analyze emails and predict whether they are spam or not. In this case, the data is categorized into two possible groups (or classes). This type of classification is also called binary classification. However, if there are more than two groups (or classes), you will be working on a multi-class classification.\n",
    "\n",
    "But what is a real-world classification problem? Consider a model that tries to predict a given user's rating for a movie where this score can only take values: like, neutral, or dislike. This is a classification problem.\n",
    "\n",
    "We will learn how to classify data using the k-nearest neighbors classifier and SVM algorithms. Just as we did for regression, we will build a classifier based on cleaned and prepared training data and test the performance of our classifier using testing data.\n",
    "\n",
    "We'll begin by looking at the fundamentals of classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fundamentals of Classification\n",
    "\n",
    "As stated earlier, the goal of any classification problem is to separate the data into relevant groups accurately using a training set. There are a lot of applications of such projects in different industries, such as education, where a model can predict whether a student will pass or fail an exam, or healthcare, where a model can assess the level of severity of a given disease for each patient.\n",
    "\n",
    "A classifier is a model that determines the label (output) or value (class) of any data point that it belongs to. For instance, suppose you have a set of observations that contains credit-worthy individuals, and another one that contains individuals that are risky in terms of their credit repayment tendencies.\n",
    "\n",
    "Let's call the first group P and the second one Q. Here is an example of such data:\n",
    "\n",
    "![Figure 3.1](img/fig3_01.jpg)\n",
    "\n",
    "With this data, you will train a classification model that will be able to correctly classify a new observation into one of these two groups (this is binary classification). The model can find patterns such as a person with a salary above \\$60,000 being less risky or that having a mortgage/income ratio above ratio 10 makes an individual more at risk of not repaying their debts. This will be a multi-class classification exercise.\n",
    "\n",
    "Classification models can be grouped into different families of algorithms. The most famous ones are as follows:\n",
    "\n",
    "  * Distance-based, such as **k-nearest neighbors**\n",
    "  * Linear models, such as **logistic regression** or **SVM**s\n",
    "  * Tree-based, such as **random forest**\n",
    "  \n",
    "In this section, you will be introduced to two algorithms from the first two types of family: k-nearest neighbors (distance-based) and SVMs (linear models).\n",
    "\n",
    "But before diving into the models, we need to clean and prepare the dataset. In the following section, we will work on a German credit approvals dataset and perform all the data preparation required for the modeling stage. Let's start by loading the data.\n",
    "\n",
    "**Go to Exercise 3.01**\n",
    "\n",
    "From the preceding output, we can see that this DataFrame has some numerical features (`int64`) but also text (`object`). We can also see that most of these features are either personal details for an individual, such as their age, or financial information such as credit history or credit amount.\n",
    "\n",
    "By completing this exercise, we have successfully loaded the data into the DataFrame and had a first glimpse of the features and information it contains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before building a classifier, we need to format our data so that we can keep relevant data in the most suitable format for classification and remove all the data that we are not interested in.\n",
    "\n",
    "The following points are the best ways to achieve this:\n",
    "\n",
    "  * Replacing or dropping values: For instance, if there are `N/A` (or `NA`) values in the dataset, we may be better off substituting these values with a numeric value we can handle. Recall from the previous chapter that `NA` stands for **Not Available** and that it represents a missing value. We may choose to ignore rows with `NA` values or replace them with an outlier value.\n",
    "  \n",
    "    > An outlier value is a value such as -1,000,000 that clearly stands out from regular values in the dataset.\n",
    "    \n",
    "    The fillna() method of a DataFrame does this type of replacement. The replacement of NA values with an outlier looks as follows:\n",
    "    \n",
    "    ```Python\n",
    "df.fillna(-1000000, inplace=True)\n",
    "    ```    \n",
    "    The `fillna()` method changes all NA values into numeric values.\n",
    "    \n",
    "    This numeric value should be far from any reasonable value in the DataFrame. Minus one million is recognized by the classifier as an exception, assuming that only positive values are there, as mentioned in the preceding note.\n",
    "    \n",
    "  * **Dropping rows or columns**: The alternative to replacing missing values with extreme values is simply dropping these rows:\n",
    "  \n",
    "    ```Python\n",
    "    df.dropna(0, inplace=True)\n",
    "    ``` \n",
    "    The first argument (value `0`) specifies that we drop rows, not columns. The second argument (`inplace=True`) specifies that we perform the drop operation without cloning the DataFrame, and will save the result in the same DataFrame. This DataFrame doesn't have any missing values, so the `dropna()` method didn't alter the DataFrame.\n",
    "  \n",
    "      > Dropping the `NA` values is less desirable, as you often lose a reasonable chunk of your dataset.\n",
    "    \n",
    "    If there is a column we do not want to include in the classification, we are better off dropping it. Otherwise, the classifier may detect false patterns in places where there is absolutely no correlation.\n",
    "  \n",
    "    For instance, your phone number itself is very unlikely to correlate with your credit score. It is a 9 to 12-digit number that may very easily feed the classifier with a lot of noise. So, we can drop the `telephone` column, as shown in the following code snippet:\n",
    "    \n",
    "      ```Python\n",
    "      df.drop(['telephone'], 1, inplace=True)\n",
    "      ```\n",
    "    The second argument (value `1`) indicates that we are dropping columns, instead of rows. The first argument is an enumeration of the columns we would like to drop (here, this is `['telephone']`). The `inplace` argument is used so that the call modifies the original DataFrame.\n",
    "    \n",
    "  * Transforming data: Often, the data format we are working with is not always optimal for the classification process. We may want to transform our data into a different format for multiple reasons, such as to highlight aspects of the data we are interested in (for example, Minmax scaling or normalization), to drop aspects of the data we are not interested in (for example, binarization), label encoding to transform categorical variables into numerical ones, and so on.\n",
    "    Minmax scaling scales each column in the data so that the lowest number in the column becomes 0, the highest number becomes 1, and all of the values in-between are proportionally scaled between 0 and 1.\n",
    "    \n",
    "    This type of operation can be performed by the MinMaxScaler method of the scikit-learn preprocessing utility, as shown in the following code snippet:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        ],\n",
       "       [0.11764706, 0.59375   ],\n",
       "       [0.        , 0.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.array([[19, 65], [4, 52], [2, 33]])\n",
    "preprocessing.MinMaxScaler(feature_range=(0,1)).fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Binarization transforms data into ones and zeros based on a condition, as shown in the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.Binarizer(threshold=10).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding example, we transformed the original data `([19, 65],[4, 52],[2, 33])` into a binary form based on the condition of whether each value is greater than $10$ or not (as defined by the `threshold=10` parameter). For instance, the first value, $19$, is above $10$, so it is replaced by $1$ in the results.\n",
    "\n",
    "Label encoding is important for preparing your features (inputs) for the modeling stage. While some of your features are string labels, scikit-learn algorithms expect this data to be transformed into numbers.\n",
    "\n",
    "This is where the `preprocessing` library of scikit-learn comes into play.\n",
    "\n",
    "  > **Note**  \n",
    "  > You might have noticed that in the credit scoring example, there were two data files. One contained labels in string form, while the other contained labels in integer form. We loaded the data with string labels so that you got some experience of how to preprocess data properly with the label encoder.\n",
    "  \n",
    "Label encoding is not rocket science. It creates a mapping between string labels and numeric values so that we can supply numbers to scikit-learn, as shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enumerate the encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Friday'),\n",
       " (1, 'Monday'),\n",
       " (2, 'Thursday'),\n",
       " (3, 'Tuesday'),\n",
       " (4, 'Wednesday')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in enumerate(label_encoder.classes_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding result shows us that scikit-learn has created a mapping for each day of the week to a respective number; for example, `Friday` will be $0$ and `Tuesday` will be $3$.\n",
    "\n",
    "  > **Note**  \n",
    "  > By default, scikit-learn assigned the mapping number by sorting the original values alphabetically. This is why `Friday` is mapped to $0$.\n",
    "  \n",
    "Now, we can use this mapping (also called an encoder) to transform data.\n",
    "\n",
    "Let's try this out on two examples, `Wednesday` and `Friday`, using the `transform()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.transform(['Wednesday', 'Friday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we got the results `4` and `0`, which are the mapping values for `Wednesday` and `Friday`, respectively.\n",
    "\n",
    "We can also use this encoder to perform the inverse transformation with the `inverse_transform` function. Let's try this with the values `0` and `4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Friday', 'Wednesday'], dtype='<U9')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.inverse_transform([0, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we got back the values `Friday` and `Wednesday`. Now, let's practice what we've learned here on the German dataset.\n",
    "\n",
    "**Go to Exercise 3.02**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dentifying Features and Labels\n",
    "Before training our model, we still have to perform two final steps. The first one is to separate our features from the label (also known as a response variable or dependent variable). The label column is the one we want our model to predict. For the German credit dataset, in our case, it will be the column called default, which tells us whether an individual will present a risk of defaulting or not.\n",
    "\n",
    "The features are all the other columns present in the dataset. The model will use the information contained in those columns and find the relevant patterns in order to accurately predict the corresponding label.\n",
    "\n",
    "The scikit-learn package requires the labels and features to be stored in two different variables. Luckily, the pandas package provides a method to extract a column from a DataFrame called `.pop()`.\n",
    "\n",
    "We will extract the `default` column and store it in a variable called `label`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      0\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "995    0\n",
       "996    0\n",
       "997    0\n",
       "998    1\n",
       "999    0\n",
       "Name: default, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/german_credit_only_numeric.csv', index_col=0)\n",
    "\n",
    "label = df.pop('default')\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we look at the content of `df`, we will see that the `default` column is not present anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['account_check_status', 'duration_in_month', 'credit_history',\n",
       "       'purpose', 'credit_amount', 'savings', 'present_emp_since',\n",
       "       'installment_as_income_perc', 'other_debtors', 'present_res_since',\n",
       "       'property', 'age', 'other_installment_plans', 'housing',\n",
       "       'credits_this_bank', 'job', 'people_under_maintenance', 'telephone',\n",
       "       'foreign_worker'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our features and labels ready, we need to split our dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing Using Scikit-Learn\n",
    "\n",
    "The final step that's required before training a classifier is to split our data into training and testing sets. We already saw how to do this using the `train_test_split` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "features_train, features_test, label_train, label_test = model_selection.train_test_split(df, label, test_size=0.1, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_split` method shuffles and then splits our features and labels into a training dataset and a testing dataset.\n",
    "\n",
    "We can specify the size of the testing dataset as a number between $0$ and $1$. A `test_size` of $0.1$ means that $10\\%$ of the data will go into the testing dataset. You can also specify a `random_state` so that you get the exact same split if you run this code again.\n",
    "\n",
    "We will use the training set to train our classifier and use the testing set to evaluate its predictive performance. By doing so, we can assess whether our model is overfitting and has learned patterns that are only relevant to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The K-Nearest Neighbors Classifier\n",
    "\n",
    "Now that we have our training and testing data, it is time to prepare our classifier to perform k-nearest neighbor classification. After being introduced to the k-nearest neighbor algorithm, we will use scikit-learn to perform classification.\n",
    "\n",
    "## Introducing the K-Nearest Neighbors Algorithm (KNN)\n",
    "\n",
    "The goal of classification algorithms is to divide data so that we can determine which data points belong to which group.\n",
    "\n",
    "Suppose that a set of classified points is given to us. Our task is to determine which class a new data point belongs to.\n",
    "\n",
    "In order to train a k-nearest neighbor classifier (also referred to as KNN), we need to provide the corresponding class for each observation on the training set, that is, which group it belongs to. The goal of the algorithm is to find the relevant relationship or patterns between the features that will lead to this class. The k-nearest neighbors algorithm is based on a proximity measure that calculates the distance between data points.\n",
    "\n",
    "The two most famous proximity (or distance) measures are the Euclidean and the Manhattan distance. We will go through more details in the next section.\n",
    "\n",
    "For any new given point, KNN will find its k nearest neighbor, see which class is the most frequent between those k neighbors, and assign it to this new observation. But what is k, you may ask? Determining the value of k is totally arbitrary. You will have to set this value upfront. This is not a parameter that can be learned by the algorithm; it needs to be set by data scientists. This kind of parameter is called a **hyperparameter**. Theoretically, you can set the value of k to between 1 and positive infinity.\n",
    "\n",
    "There are two main best practices to take into consideration:\n",
    "\n",
    "  * k should always be an odd number. The reason behind this is that we want to avoid a situation that ends in a tie. For instance, if you set k=4 and it so happens that two of the neighbors of a point are from class A and the other two are from class B, then KNN doesn't know which class to choose. To avoid this situation, it is better to choose k=3 or k=5.\n",
    "  * The greater k is, the more accurate KNN will be. For example, if we compare the cases between k=1 and k=15, the second one will give you more confidence that KNN will choose the right class as it will need to look at more neighbors before making a decision. On the other hand, with k=1, it only looks at the closest neighbor and assigns the same class to an observation. But how can we be sure it is not an outlier or a special case? Asking more neighbors will lower the risk of making the wrong decision. But there is a drawback to this: the higher k is, the longer it will take KNN to make a prediction. This is because it will have to perform more calculations to get the distance between all the neighbors of an observation. Due to this, you have to find the sweet spot that will give correct predictions without compromising too much on the time it takes to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics With K-Nearest Neighbors Classifier in Scikit-Learn\n",
    "\n",
    "Many distance metrics could work with the k-nearest neighbors algorithm. We will present the two most frequently used ones: the Euclidean distance and the Manhattan distance of two data points.\n",
    "\n",
    "### The Euclidean Distance\n",
    "\n",
    "The distance between two points, $A$ and $B$, with the coordinates $A=(a_1, a_2, \\dots, a_n)$ and $B=(b_1, b_2, \\dots, b_n)$, respectively, is the length of the line connecting these two points. For example, if A and B are two-dimensional data points, the Euclidean distance, $d$, will be as follows:\n",
    "\n",
    "![Figure 3.7](img/fig3_07.jpg)\n",
    "\n",
    "The formula to calculate the Euclidean distance is as follows:\n",
    "\n",
    "$$\n",
    "distance\\left( a, b \\right) = \\sqrt{\\sum_{i=1}^n \\left( a_i - b_i \\right)^2}\n",
    "$$\n",
    "\n",
    "As we will be using the Euclidean distance in this book, let's see how we can use scikit-learn to calculate the distance of multiple points.\n",
    "\n",
    "We have to import `euclidean_distances` from `sklearn.metrics.pairwise`. This function accepts two sets of points and returns a matrix that contains the pairwise distance of each point from the first and second sets of points.\n",
    "\n",
    "Let's take the example of an observation, Z, with coordinates $(4, 4)$. Here, we want to calculate the Euclidean distance with 3 others points, $A$, $B$, and $C$, with the coordinates $(2, 3)$, $(3, 7)$, and $(1, 6)$, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.23606798, 3.16227766, 3.60555128]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "observation = [4, 4]\n",
    "neighbors = [[2, 3], [3, 7], [1, 6]]\n",
    "euclidean_distances([observation], neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the distance of $Z=(4,4)$ and $B=(3,7)$ is approximately $3.162$, which is what we got in the output.\n",
    "\n",
    "We can also calculate the Euclidean distances between points in the same set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 4.12310563, 3.16227766],\n",
       "       [4.12310563, 0.        , 2.23606798],\n",
       "       [3.16227766, 2.23606798, 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal that contains value $0$ corresponds to the Euclidean distance between each data point and itself. This matrix is symmetric from this diagonal as it calculates the distance of two points and its reverse. For example, the value $4.12310563$ on the first row is the distance between A and B, while the same value on the second row corresponds to the distance between B and A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Manhattan/Hamming Distance\n",
    "\n",
    "The formula of the Manhattan (or Hamming) distance is very similar to the Euclidean distance, but rather than using the square root, it relies on calculating the absolute value of the difference of the coordinates of the data points:\n",
    "\n",
    "$$\n",
    "distance \\left( a, b  \\right) = \\sum_{i=1}^n \\left| a_i - b_i \\right|\n",
    "$$\n",
    "\n",
    "You can think of the Manhattan distance as if we're using a grid to calculate the distance rather than using a straight line:\n",
    "\n",
    "![Figure 3.10](img/fig3_10.jpg)\n",
    "\n",
    "As shown in the preceding plot, the Manhattan distance will follow the path defined by the grid to point B from A.\n",
    "\n",
    "Another interesting property is that there can be multiple shortest paths between A and B, but their Manhattan distances will all be equal to each other. In the preceding example, if each cell of the grid equals a unit of 1, then all three of the shortest paths highlighted will have a Manhattan distance of 9.\n",
    "\n",
    "The Euclidean distance is a more accurate generalization of distance, while the Manhattan distance is slightly easier to calculate as you only need to find the difference between the absolute value rather than calculating the difference between squares and then taking the root.\n",
    "\n",
    "**Go to Exercise 3.03**\n",
    "\n",
    "From the final step in the Exercise 3.03, we can see that the three nearest neighbors are as follows:\n",
    "\n",
    "  * $0.1564897$ for point $[0.6, 0.37037037, 1.]$\n",
    "  * $0.17114358$ for point $[0.6, 0.11111111, 0.]$\n",
    "  * $0.32150303$ for point $[0.6, 0.55555556, 1.]$\n",
    "  \n",
    "If we choose `k=3`, KNN will look at the classes for these three nearest neighbors and since two of them have a label of $1$, it will assign this class to our new observation, $[0.5, 0.26]$. This means that our three-nearest neighbors classifier will classify this new employee as being more likely to stay for at least 2 years.\n",
    "\n",
    "By completing this exercise, we saw how a KNN classifier will classify a new observation by finding its three closest neighbors using the Euclidean distance and then assign the most frequent class to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterization of the K-Nearest Neighbors Classifier in scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameterization of the classifier is where you fine-tune the accuracy of your classifier. Since we haven't learned all of the possible variations of k-nearest neighbors, we will concentrate on the parameters that you will understand based on this topic:\n",
    "\n",
    "  > You can access the documentation of the k-nearest neighbors classifier [here](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "  \n",
    "  * n_neighbors: This is the k value of the k-nearest neighbors algorithm. The default value is 5.\n",
    "  * metric: When creating the classifier, you will see a name – `Minkowski`. Don't worry about this name – you have learned about the first- and second-order Minkowski metrics already. This metric has a `power` parameter. For `p=1`, the Minkowski metric is the same as the Manhattan metric. For `p=2`, the Minkowski metric is the same as the Euclidean metric.\n",
    "  * p: This is the power of the Minkowski metric. The default value is 2.\n",
    "  \n",
    "You have to specify these parameters once you create the classifier:\n",
    "\n",
    "```Python\n",
    "classifier = neighbors.KNeighborsClassifier(n_neighbors=50, p=2)\n",
    "```\n",
    "\n",
    "Then, you will have to fit the KNN classifier with your training data:\n",
    "\n",
    "```Python\n",
    "classifier.fit(features, label)\n",
    "```\n",
    "\n",
    "The `predict()` method can be used to predict the label for any new data point:\n",
    "\n",
    "```Python\n",
    "classifier.predict(new_data_point)\n",
    "```\n",
    "\n",
    "**Got to Exercise 3.04**\n",
    "\n",
    "In the exercise 3.04, we learned how to split a dataset into training and testing sets and fit a KNN algorithm. Our final model can accurately predict whether an individual is more likely to default or not $75\\%$ of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Support Vector Machines\n",
    "\n",
    "We first used SVMs for regression in the section \"An Introduction to Regression\". In this topic, you will find out how to use SVMs for classification. As always, we will use scikit-learn to run our examples in practice.\n",
    "\n",
    "## What Are Support Vector Machine Classifiers?\n",
    "\n",
    "The goal of an SVM is to find a surface in an n-dimensional space that separates the data points in that space into multiple classes.\n",
    "\n",
    "In two dimensions, this surface is often a straight line. However, in three dimensions, the SVM often finds a plane. These surfaces are optimal in the sense that they are based on the information available to the machine so that it can optimize the separation of the n-dimensional spaces.\n",
    "\n",
    "The optimal separator found by the SVM is called the best separating hyperplane.\n",
    "\n",
    "An SVM is used to find one surface that separates two sets of data points. In other words, SVMs are **binary classifiers**. This does not mean that SVMs can only be used for binary classification. Although we were only talking about one plane, SVMs can be used to partition a space into any number of classes by generalizing the task itself.\n",
    "\n",
    "The separator surface is optimal in the sense that it maximizes the distance of each data point from the separator surface.\n",
    "\n",
    "A vector is a mathematical structure defined on an n-dimensional space that has a magnitude (length) and a direction. In two dimensions, you draw the vector (x, y) from the origin to the point (x, y). Based on geometry, you can calculate the length of the vector using the Pythagorean theorem and the direction of the vector by calculating the angle between the horizontal axis and the vector.\n",
    "\n",
    "For instance, in two dimensions, the vector (3, -4) has the following magnitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.sqrt(3 * 3 + 4 * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the following direction (in degrees):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-53.13010235415597"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arctan(-4/3) / 2 / np.pi * 360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Support Vector Machines\n",
    "\n",
    "Suppose that two sets of points with two different classes, 0 and 1, are given. For simplicity, we can imagine a two-dimensional plane with two features: one mapped on the horizontal axis and one mapped on the vertical axis.\n",
    "\n",
    "The objective of the SVM is to find the best separating line that separates points $A, D, C, B$, and $H$, which all belong to class **0**, from points $E, F$, and $G$, which are of class **1**:\n",
    "\n",
    "![Figure 3.13](img/fig3_13.jpg)\n",
    "\n",
    "But separation is not always that obvious. For instance, if there is a new point of class 0 in-between $E, F$, and $G$, there is no line that could separate all the points without causing errors. If the points from class 0 form a full circle around the class 1 points, there is no straight line that could separate the two sets:\n",
    "\n",
    "![Figure 3.14](img/fig3_14.jpg)\n",
    "\n",
    "For instance, in the preceding graph, we tolerate two outlier points, $O$ and $P$.\n",
    "\n",
    "In the following solution, we do not tolerate outliers, and instead of a line, we create the best separating path consisting of two half-lines:\n",
    "\n",
    "![Figure 3.15](img/fig3_15.jpg)\n",
    "\n",
    "The perfect separation of all data points is rarely worth the resources. Therefore, the SVM can be regularized to simplify and restrict the definition of the best separating shape and allow outliers.\n",
    "\n",
    "The regularization parameter of an SVM determines the rate of errors to allow or forbid misclassifications.\n",
    "\n",
    "An SVM has a kernel parameter. A linear kernel strictly uses a linear equation to describe the best separating hyperplane. A polynomial kernel uses a polynomial, while an exponential kernel uses an exponential expression to describe the hyperplane.\n",
    "\n",
    "A margin is an area centered around the separator and is bounded by the points closest to the separator. A balanced margin has points from each class that are equidistant from the line.\n",
    "\n",
    "When it comes to defining the allowed error rate of the best separating hyperplane, a gamma parameter decides whether only the points near the separator count in determining the position of the separator, or whether the points farthest from the line count, too. The higher the gamma, the lower the number of points that influence the location of the separator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines in scikit-learn\n",
    "\n",
    "Our entry point is the end result of Activity 3.01. Once we have split the training and test data, we are ready to set up the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_url = 'data/german_prepared.csv'\n",
    "df = pd.read_csv(file_url)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "scaled_credit = scaler.fit_transform(df)\n",
    "label = scaled_credit[:, 0]\n",
    "features = scaled_credit[:, 1:]\n",
    "\n",
    "features_train, features_test, label_train, label_test = train_test_split(features,\n",
    "                                                                         label, test_size=0.2, random_state=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the k-nearest neighbors classifier, we will use the `svm.SVC()` classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(features_train, label_train)\n",
    "classifier.score(features_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the default SVM classifier of scikit-learn does a better job than the k-nearest neighbors classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the scikit-learn SVM\n",
    "\n",
    "The following are the parameters of the scikit-learn SVM:\n",
    "\n",
    "  * `kernel`: This is a string or callable parameter specifying the kernel that's being used in the algorithm. The predefined kernels are `linear`, `poly`, `rbf`, `sigmoid`, and `precomputed`. The default value is `rbf`.\n",
    "  * `degree`: When using a polynomial, you can specify the degree of the polynomial. The default value is $3$.\n",
    "  * `gamma`: This is the kernel coefficient for `rbf`, `poly`, and `sigmoid`. The default value is `auto`, which is computed as $\\frac{1}{number_of_features}$.\n",
    "  * `C`: This is a floating-point number with a default of $1.0$ that describes the penalty parameter of the error term.\n",
    "  \n",
    "Here is an example of an SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.745"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_2 = svm.SVC(kernel=\"poly\", C=2, degree=4, gamma=0.05)\n",
    "classifier_2.fit(features_train, label_train)\n",
    "classifier_2.score(features_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
