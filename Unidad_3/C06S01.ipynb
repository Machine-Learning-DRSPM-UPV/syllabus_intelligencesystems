{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning\n",
    "\n",
    "In this section, you will be introduced to the final topic on neural networks and deep learning. You will be learning about TensorFlow, Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs). You will use key deep learning concepts to determine creditworthiness of individuals and predict housing prices in a neighborhood. Later on, you will also implement an image classification program using the skills you learned. By the end of this chapter, you will have a firm grasp on the concepts of neural networks and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Previously, we learned about what clustering problems are and saw several algorithms, such as k-means, that can automatically group data points on their own. We will learn about neural networks and deep learning networks.\n",
    "\n",
    "The difference between neural networks and deep learning networks is the complexity and depth of the networks. Traditionally, neural networks have only one hidden layer, while deep learning networks have more than that.\n",
    "\n",
    "Although we will use neural networks and deep learning for supervised learning, note that neural networks can also model unsupervised learning techniques. This kind of model was actually quite popular in the 1980s, but because the computation power required was limited at the time, it's only recently that this model has been widely adopted. With the democratization of Graphics Processing Units (GPUs) and cloud computing, we now have access to a tremendous amount of computation power. This is the main reason why neural networks and especially deep learning are hot topics again.\n",
    "\n",
    "Deep learning can model more complex patterns than traditional neural networks, and so deep learning is more widely used nowadays in computer vision (in applications such as face detection and image recognition) and natural language processing (in applications such as chatbots and text generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neurons\n",
    "\n",
    "Artificial Neural Networks (ANNs), as the name implies, try to replicate how a human brain works, and more specifically how neurons work.\n",
    "\n",
    "A neuron is a cell in the brain that communicates with other cells via electrical signals. Neurons can respond to stimuli such as sound, light, and touch. They can also trigger actions such as muscle contractions. On average, a human brain contains 10 to 20 billion neurons. That's a pretty huge network, right? This is the reason why humans can achieve so many amazing things. This is also why researchers have tried to emulate how the brain operates and in doing so created ANNs.\n",
    "\n",
    "ANNs are composed of multiple artificial neurons that connect to each other and form a network. An artificial neuron is simply a processing unit that performs mathematical operations on some inputs ($x_1, x_2, \\dots, x_n$) and returns the final results (`y`) to the next unit, as shown here:\n",
    "\n",
    "![Figure 6.1](img/fig6_01.jpg)\n",
    "\n",
    "We will see how an artificial neuron works more in detail in the coming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons in TensorFlow\n",
    "\n",
    "TensorFlow is currently the most popular neural network and deep learning framework. It was created and is maintained by Google. TensorFlow is used for voice recognition and voice search, and it is also the brain behind translate.google.com. Later in this chapter, we will use TensorFlow to recognize written characters.\n",
    "\n",
    "The TensorFlow API is available in many languages, including Python, JavaScript, Java, and C. TensorFlow works with **tensors**. You can think of a tensor as a container composed of a matrix (usually with high dimensions) and additional information related to the operations it will perform (such as weights and biases, which you will be looking at later in this chapter). A tensor with no dimensions (with no rank) is a scalar. A tensor of rank 1 is a vector, rank 2 tensors are matrices, and a rank 3 tensor is a three-dimensional matrix. The rank indicates the dimensions of a tensor. In this chapter, we will be looking at tensors of ranks 2 and 3.\n",
    "\n",
    "  > Mathematicians use the terms matrix and dimension, whereas deep learning programmers use tensor and rank instead.\n",
    "\n",
    "TensorFlow also comes with mathematical functions to transform tensors, such as the following:\n",
    "\n",
    "  * Arithmetic operations: `add` and `multiply`\n",
    "  * Exponential operations: `exp` and `log`\n",
    "  * Relational operations: `greater`, `less`, and `equal`\n",
    "  * Array operations: `concat`, `slice`, and `split`\n",
    "  * Matrix operations: `matrix_inverse`, `matrix_determinant`, and `matmul`\n",
    "  * Non-linear operations: `sigmoid`, `relu`, and `softmax`\n",
    "  \n",
    "**Go to Exercise 6.1**\n",
    "\n",
    "In the Exercise 6.1, you successfully implemented an artificial neuron using TensorFlow. This is the base of any neural network model using tensorflow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "Neural networks aren't the newest branch of **Artificial Intelligence (AI)**. Neural networks are inspired by how the human brain works. They were invented in the 1940s by Warren McCulloch and Walter Pitts. The neural network was a mathematical model that was used to describe how the human brain can solve problems.\n",
    "\n",
    "We will use ANN to refer to both the mathematical model, and the biological neural network when talking about the human brain.\n",
    "\n",
    "The way a neural network learns is more complex compared to other classification or regression models. The neural network model has a lot of internal variables, and the relationship between the input and output variables may involve multiple internal layers. Neural networks have higher accuracy than other supervised learning algorithms.\n",
    "\n",
    "  > **Note**  \n",
    "  > Mastering neural networks with TensorFlow is a complex process. The purpose of this section is to provide you with an introductory resource to get started.\n",
    "\n",
    "The main example we are going to use is the recognition of digits from an image. We are considering this format since each image is small, and we have around 70,000 images available. The processing power required to process these images is similar to that of a regular computer.\n",
    "\n",
    "ANNs work similarly to how the human brain works. A dendroid in a human brain is connected to a nucleus, and the nucleus is connected to an axon. In an ANN, the input is the dendroid, where the calculations occur is the nucleus, and the output is the axon.\n",
    "\n",
    "An artificial neuron is designed to replicate how a nucleus works. It will transform an input signal by calculating a matrix multiplication followed by an activation function. If this function determines that a neuron has to fire, a signal appears in the output. This signal can be the input of other neurons in the network:\n",
    "\n",
    "![Figure 6.2](img/fig6_02.jpg)\n",
    "\n",
    "Let's understand the preceding figure further by taking the example of $n=4$. In this case, the following applies:\n",
    "\n",
    "  * $X$ is the input matrix, which is composed of $x_1, x_2, x_3$, and $x_4$.\n",
    "  * $W$, the weight matrix, will be composed of $w_1, w_2, w_3$, and $w_4$.\n",
    "  * $b$ is the bias.\n",
    "  * $f$ is the activation function.\n",
    "\n",
    "We will first calculate Z (the left-hand side of the neuron) with matrix multiplication and bias:\n",
    "\n",
    "$$\n",
    "Z = W * X + b = x_1 * w_1 + x_2 * w_2 + x_3 * w_3 + x_4 * w_4 + b\n",
    "$$\n",
    "\n",
    "Then the output, `y`, will be calculated by applying a function, `f`:\n",
    "\n",
    "$$\n",
    "y = f(Z) = f(x_1 * w_1 + x_2 * w_2 + x_3 * w_3 + x_4 * w_4 + b)\n",
    "$$\n",
    "\n",
    "Great – this is how an artificial neuron works under the hood. It is two matrix operations, a product followed by a sum, and a function transformation.\n",
    "\n",
    "We now move on to the next section – weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights\n",
    "\n",
    "**W** (also called the weight matrix) refers to weights, which are parameters that are automatically learned by neural networks in order to predict accurately the output, `y`.\n",
    "\n",
    "A single neuron is the combination of the weighted sum and the activation function and can be referred to as a hidden layer. A neural network with one hidden layer is called a **regular neural network**:\n",
    "\n",
    "![Figure 6.3](img/fig6_03.jpg)\n",
    "\n",
    "When connecting inputs and outputs, we may have multiple hidden layers. A neural network with multiple layers is called a **deep neural network**.\n",
    "\n",
    "The term deep learning comes from the presence of multiple layers. When creating an **Artificial Neural Network (ANN)**, we can specify the number of hidden layers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biases\n",
    "Previously, we saw that the equation for a neuron is as follows:\n",
    "\n",
    "$$\n",
    "y = f(x_1 * w_1 + x_2 * w_2 + x_3 * w_3 + x_4 * w_4)\n",
    "$$\n",
    "\n",
    "The problem with this equation is that there is no constant factor that depends on the inputs $x_1, x_2, x_3, and x_4$. The preceding equation can model any linear function that will go through the point 0: if all $w$ values are equal to 0 then $y$ will also equal to 0. But what about other functions that don't go through the point 0? For example, imagine that we are predicting the probability of churn for an employee by their month of tenure. Even if they haven't worked for the full month yet, the probability of churn is not zero.\n",
    "\n",
    "To accommodate this situation, we need to introduce a new parameter called **bias**. It is a constant that is also referred to as the **intercept**. Using the churn example, the bias `b` can equal to 0.5 and therefore the churn probability for a new employer during the first month will be 50%.\n",
    "\n",
    "Therefore, we add bias to the equation:\n",
    "\n",
    "$$\n",
    "y = f(x_1 * w_1 + x_2 * w_2 + x_3 * w_3 + x_4 * w_4 + b) \\\\\n",
    "y = f(X \\cdot W + b)\n",
    "$$\n",
    "\n",
    "The first equation is the verbose form, describing the role of each coordinate, weight coefficient, and bias. The second equation is the vector form, where $x = (x_1, x_2, x_3, x_4)$ and $w = (w_1, w_2, w_3, w_4)$. The dot operator between the vectors symbolizes the dot or scalar product of the two vectors. The two equations are equivalent. We will use the second form in practice because it is easier to define a vector of variables using TensorFlow than to define each variable one by one.\n",
    "\n",
    "Similarly, for $w_1, w_2, w_3, and w_4$, the bias, `b`, is a variable, meaning that its value can change during the learning process.\n",
    "\n",
    "With this constant factor built into each neuron, a neural network model becomes more flexible in terms of fitting a specific training dataset better.\n",
    "\n",
    "  > It may happen that the product $p = x_1*w_1 + x_2*w_2 + x_3*w_3 + x_4*w_4$ is negative due to the presence of a few negative weights. We may still want to give the model the flexibility to execute (or fire) a neuron with values above a given negative number. Therefore, adding a constant bias, $b = 5$, for instance, can ensure that the neuron fires for values between -5 and 0 as well.\n",
    "  \n",
    "TensorFlow provides the `Dense()` class to model the hidden layer of a neural network (also called the fully connected layer):\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras import layers\n",
    "layer1 = layers.Dense(units=128, input_shape=[200])\n",
    "```\n",
    "\n",
    "In this example, we have created a fully connected layer of 128 neurons that takes as input a tensor of shape 200.\n",
    "\n",
    "  > You can find more information on this TensorFlow class at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense).\n",
    "  \n",
    "The `Dense()` class is expected to have a flattened input (only one row). For instance, if your input is of shape `28` by `28`, you will have to flatten it beforehand with the Flatten() class in order to get a single row with 784 neurons (`28 * 28`):\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras import layers\n",
    "input_layer = layers.Flatten(input_shape=(28, 28))\n",
    "layer1 = layers.Dense(units=128)\n",
    "```\n",
    "\n",
    "  > You can find more information on this TensorFlow class at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten).\n",
    "  \n",
    "In the following sections, we will learn about how we can extend this layer of neurons with additional parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases for ANNs\n",
    "\n",
    "ANNs have their place among supervised learning techniques. They can model both classification and regression problems. A classifier neural network seeks a relationship between features and labels. The features are the input variables, while each class the classifier can choose as a return value is a separate output. In the case of regression, the input variables are the features, while there is one single output: the predicted value. While traditional classification and regression techniques have their use cases in AI, ANNs are generally better at finding complex relationships between inputs and outputs.\n",
    "\n",
    "In the next section, we will be looking at activation functions and their different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "As seen previously, a single neuron needs to perform a transformation by applying an activation function. Different activation functions can be used in neural networks. Without these functions, a neural network would simply be a linear model that could easily be described using matrix multiplication.\n",
    "\n",
    "The activation function of a neural network provides non-linearity and therefore can model more complex patterns. Two very common activation functions are sigmoid and tanh (the hyperbolic tangent function).\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "The formula of `sigmoid` is as follows:\n",
    "\n",
    "$$\n",
    "f(x) = \\sigma (x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "The output values of a sigmoid function range from 0 to 1. This activation function is usually used at the last layer of a neural network for a binary classification problem.\n",
    "\n",
    "### Tanh\n",
    "\n",
    "The formula of the hyperbolic tangent is as follows:\n",
    "\n",
    "$$\n",
    "f(x) = tanh(x) = \\frac{(e^x - e^{-x})}{(e^x + e^{-x})}\n",
    "$$\n",
    "\n",
    "The `tanh` activation function is very similar to the `sigmoid` function and was quite popular until recently. It is usually used in the hidden layers of a neural network. Its values range between -1 and 1.\n",
    "\n",
    "### ReLU\n",
    "\n",
    "Another important activation function is `relu`. **ReLU** stands for **Rectified Linear Unit**. It is currently the most widely used activation function for hidden layers. Its formula is as follows:\n",
    "\n",
    "$$\n",
    "f(x) = \\left\\{\\begin{matrix}\n",
    " 0 & \\text{for x} & \\leq 0 \\\\ \n",
    " x & \\text{for x} & > 0 \n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "There are now different variants of `relu` functions, such as `leaky ReLU` and `PReLU`.\n",
    "\n",
    "### Softmax\n",
    "\n",
    "The function shrinks the values of a list to be between 0 and 1 so that the sum of the elements of the list becomes 1. The definition of the `softmax` function is as follows:\n",
    "\n",
    "$$\n",
    "f_i \\left(\\overrightarrow{x} \\right) = \\frac{e^{x_i}}{\\sum^J_{j=1} e^{x_j}} \\text{for i} = 1, \\dots, J\n",
    "$$\n",
    "\n",
    "The `softmax` function is usually used as the last layer of a neural network for multi-class classification problems as it can generate probabilities for each of the different output classes.\n",
    "\n",
    "Remember, in TensorFlow, we can extend a `Dense()` layer with an activation function; we just need to set the `activation` parameter. In the following example, we will add the `relu` activation function:\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras import layers\n",
    "layer1 = layers.Dense(units=128, input_shape=[200], activation='relu')\n",
    "```\n",
    "\n",
    "**Go to Exercise 6.2**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation and the Loss Function\n",
    "\n",
    "So far, we have seen how a neuron can take an input and perform some mathematical operations on it and get an output. We learned that a neural network is a combination of multiple layers of neurons.\n",
    "\n",
    "The process of transforming the inputs of a neural network into a result is called **forward propagation** (or the forward pass). What we are asking the neural network to do is to make a prediction (the final output of the neural network) by applying multiple neurons to the input data:\n",
    "\n",
    "![Figure 6.11](img/fig6_11.jpg)\n",
    "\n",
    "The neural network relies on the weights matrices, biases, and activation function of each neuron to calculate the predicted output value, $(\\hat{y})$. For now, let's assume the values of the weight matrices and biases are set in advance. The activation functions are defined when you design the architecture of the neural networks.\n",
    "\n",
    "As for any supervised machine learning algorithm, the goal is to make accurate predictions. This implies that we need to assess how accurate the predictions are compared to the true values. For traditional machine learning algorithms, we used scoring metrics such as mean squared error, accuracy, or the F1 score. This can also be applied to neural networks, but the only difference is that such scores are used in two different ways:\n",
    "\n",
    "  * They are used by data scientists to assess the performance of a model on training and testing sets and then tune hyperparameters if needed. This also applies to neural networks, so nothing new here.\n",
    "  * They are used by neural networks to automatically learn from mistakes and update weight matrices and biases. This will be explained in more detail in the next section, which is about backpropagation. So, the neural network will use a metric (also called a **loss function**) to compare its predicted values, $(\\hat{y})$ to the true label, $y$, and then learn how to make better predictions automatically.\n",
    "  \n",
    "The loss function is critical to a neural network learning to make good predictions. This is a hyperparameter that needs to be defined by data scientists while designing the architecture of a neural network. The choice of which loss function to use is totally arbitrary and depending on the dataset or the problem you want to solve, you will pick one or another. Luckily for us, though, there are some basic rules of thumb that work in most cases:\n",
    "\n",
    "  * If you are working on a regression problem, you can use mean squared error.\n",
    "  * If it is a binary classification, the loss function should be binary cross-entropy.\n",
    "  * If it is a multi-class classification, then categorical cross-entropy should be your go-to choice.\n",
    "  \n",
    "As a final note, the choice of loss function will also define which activation function you will have to use on the last layer of the neural network. Each loss function expects a certain type of data in order to properly assess prediction performance.\n",
    "\n",
    "Here is the list of activation functions according to the loss function and type of project/problem:\n",
    "\n",
    "| Problem Type   | Last-Layer Actvivation Function  | Loss Function  |\n",
    "|---|---|---|\n",
    "| Regression   | None (or identity function)  | Mean squared error   |\n",
    "| Binary classification   | sigmoid   | Binary cross-entropy   |\n",
    "| Multi-class classification   | softmax   | Categorical cross-entropy   |\n",
    "\n",
    "With TensorFlow, in order to build your custom architecture, you can instantiate the `Sequential()` class and add your layers of fully connected neurons as shown in the following code snippet:\n",
    "\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential()\n",
    "input_layer = layers.Flatten(input_shape=(28,28))\n",
    "layer1 = layers.Dense(128, activation='relu')\n",
    "model.add(input_layer)\n",
    "model.add(layer1)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Previously, we learned how a neural network makes predictions by using weight matrices and biases (we can combine them into a single matrix) from its neurons. Using the loss function, a network determines how good or bad the predictions are. It would be great if it could use this information and update the parameters accordingly. This is exactly what backpropagation is about: optimizing a neural network's parameters.\n",
    "\n",
    "Training a neural network involves executing forward propagation and backpropagation multiple times in order to make predictions and update the parameters from the errors. During the first pass (or propagation), we start by initializing all the weights of the neural network. Then, we apply forward propagation, followed by backpropagation, which updates the weights.\n",
    "\n",
    "We apply this process several times and the neural network will optimize its parameters iteratively. You can decide to stop this learning process by setting the maximum number of times the neural networks will go through the entire dataset (also called epochs) or define an early stop threshold if the neural network's score is not improving anymore after few epochs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers and the Learning Rate\n",
    "\n",
    "In the previous section, we saw that a neural network follows an iterative process to find the best solution for any input dataset. Its learning process is an optimization process. You can use different optimization algorithms (also called **optimizers**) for a neural network. The most popular ones are `Adam`, `SGD`, and `RMSprop`.\n",
    "\n",
    "One important parameter for the neural networks optimizer is the learning rate. This value defines how quickly the neural network will update its weights. Defining a too-low learning rate will slow down the learning process and the neural network will take a long time before finding the right parameters. On the other hand, having too-high a learning rate can make the neural network not learn a solution as it is making bigger weight changes than required. A good practice is to start with a not-too-small learning rate (such as 0.01 or 0.001), then stop the neural network training once its score starts to plateau or get worse, and lower the learning rate (by an order of magnitude, for instance) and keep training the network.\n",
    "\n",
    "With TensorFlow, you can instantiate an optimizer from `tf.keras.optimizers`. For instance, the following code snippet shows us how to create an `Adam` optimizer with 0.001 as the learning rate and then compile our neural network by specifying the loss function (`'sparse_categorical_crossentropy'`) and metrics to be displayed (`'accuracy'`):\n",
    "\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Once the model is compiled, we can then train the neural network with the `.fit()` method like this:\n",
    "\n",
    "```Python\n",
    "model.fit(features_train, label_train, epochs=5)\n",
    "```\n",
    "\n",
    "Here we trained the neural network on the training set for 5 epochs. Once trained, we can use the model on the testing set and assess its performance with the `.evaluate()` method:\n",
    "\n",
    "```Python\n",
    "model.evaluate(features_test, label_test)\n",
    "```\n",
    "\n",
    "**Go to Exercise 6.03**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "As with any machine learning algorithm, neural networks can face the problem of overfitting when they learn patterns that are only relevant to the training set. In such a case, the model will not be able to generalize the unseen data.\n",
    "\n",
    "Luckily, there are multiple techniques that can help reduce the risk of overfitting:\n",
    "\n",
    "  * L1 regularization, which adds a penalty parameter (absolute value of the weights) to the loss function\n",
    "  * L2 regularization, which adds a penalty parameter (squared value of the weights) to the loss function\n",
    "  * Early stopping, which stops the training if the error for the validation set increases while the error decreases for the training set\n",
    "  * Dropout, which will randomly remove some neurons during training\n",
    "  \n",
    "All these techniques can be added at each layer of a neural network we create.\n",
    "\n",
    "**Go to Exercise 6.4**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "Now that we are comfortable in building and training a neural network with one hidden layer, we can look at more complex architecture with deep learning.\n",
    "\n",
    "Deep learning is just an extension of traditional neural networks but with deeper and more complex architecture. Deep learning can model very complex patterns, be applied in tasks such as detecting objects in images and translating text into a different language.\n",
    "\n",
    "### Shallow versus Deep Networks\n",
    "\n",
    "Now that we are comfortable in building and training a neural network with one hidden layer, we can look at more complex architecture with deep learning.\n",
    "\n",
    "As mentioned earlier, we can add more hidden layers to a neural network. This will increase the number of parameters to be learned but can potentially help to model more complex patterns. This is what deep learning is about: increasing the depth of a neural network to tackle more complex problems.\n",
    "\n",
    "For instance, we can add a second layer to the neural network we presented earlier in the section on forward propagation and loss functions:\n",
    "\n",
    "![Figure 6.19](img/fig6_19.jpg)\n",
    "\n",
    "In theory, we can add an infinite number of hidden layers. But there is a drawback with deeper networks. Increasing the depth will also increase the number of parameters to be optimized. So, the neural network will have to train for longer. So, as good practice, it is better to start with a simpler architecture and then steadily increase its depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision and Image Classification\n",
    "\n",
    "Deep learning has achieved amazing results in computer vision and natural language processing. Computer vision is a field that involves analyzing digital images. A digital image is a matrix composed of **pixels**. Each pixel has a value between $0$ and $255$ and this value represents the intensity of the pixel. An image can be black and white and have only one channel. But it can also have colors, and in that case, it will have three channels for the colors red, green, and blue. This digital version of an image that can be fed to a deep learning model.\n",
    "\n",
    "There are multiple applications of computer vision, such as image classification (recognizing the main object in an image), object detection (localizing different objects in an image), and image segmentation (finding the edges of objects in an image). In this book, we will only look at image classification.\n",
    "\n",
    "In the next section, we will look at a specific type of architecture: CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are ANNs that are optimized for image-related pattern recognition. CNNs are based on convolutional layers instead of fully connected layers.\n",
    "\n",
    "A convolutional layer is used to detect patterns in an image with a filter. A filter is just a matrix that is applied to a portion of an input image through a convolutional operation and the output will be another image (also called a feature map) with the highlighted patterns found by the filter. For instance, a simple filter can be one that recognizes vertical lines on a flower, such as for the following image:\n",
    "\n",
    "![Figure 6.20](img/fig6_20.jpg)\n",
    "\n",
    "These filters are not set in advance but learned by CNNs automatically. After the training is over, a CNN can recognize different shapes in an image. These shapes can be anywhere on the image, and the convolutional operator recognizes similar image information regardless of its exact position and orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Operations\n",
    "\n",
    "A convolution is a specific type of matrix operation. For an input image, a filter of size $n*n$ will go through a specific area of an image and apply an element-wise product and a sum and return the calculated value:\n",
    "\n",
    "![Figure 6.21](img/fig6_21.jpg)\n",
    "\n",
    "In the preceding example, we applied a filter to the top-left part of the image. Then we applied an element-wise product that just multiplied an element from the input image to the corresponding value on the filter. In the example, we calculated the following:\n",
    "\n",
    "  * 1st row, 1st column: $5 * 2 = 10$\n",
    "  * 1st row, 2nd column: $10 * 0 = 0$\n",
    "  * 1st row, 3rd column: $15 * (-1) = -15$\n",
    "  * 2nd row, 1st column: $10 * 2 = 20$\n",
    "  * 2nd row, 2nd column: $20 * 0 = 0$\n",
    "  * 2nd row, 3rd column: $30 * (-1) = -30$\n",
    "  * 3rd row, 1st column: $100 * 2 = 200$\n",
    "  * 3rd row, 2nd column: $150 * 0 = 0$\n",
    "  * 3rd row, 3rd column: $200 * (-1) = -200$\n",
    "\n",
    "Finally, we perform the sum of these values: $10 + 0 -15 + 20 + 0 - 30 + 200 + 0 - 200 = -15$.\n",
    "\n",
    "Then we will perform the same operation by sliding the filter to the right by one column from the input image. We keep sliding the filter until we have covered the entire image:\n",
    "\n",
    "![Figure 6.22](img/fig6_22.jpg)\n",
    "\n",
    "Rather than sliding column by column, we can also slide by two, three, or more columns. The parameter defining the length of this sliding operation is called the **stride**.\n",
    "\n",
    "You may have noticed that the result of the convolutional operation is an image (or feature map) with smaller dimensions than the input image. If you want to keep the exact same dimensions, you can add additional rows and columns with the value 0 around the border of the input image. This operation is called **padding**.\n",
    "\n",
    "This is what is behind a convolutional operation. A convolutional layer is just the application of this operation with multiple filters.\n",
    "\n",
    "We can declare a convolutional layer in TensorFlow with the following code snippet:\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras import layers\n",
    "layers.Conv2D(32, kernel_size=(3, 3), strides=(1,1), padding=\"valid\", activation=\"relu\")\n",
    "```\n",
    "\n",
    "In the preceding example, we have instantiated a convolutional layer with $32$ filters (also called **kernels**) of size $(3, 3)$ with stride of $1$ (sliding window by 1 column or row at a time) and no padding (`padding=\"valid\"`).\n",
    "\n",
    "  > You can read more about this Conv2D class on TensorFlow's website, at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).\n",
    "  \n",
    "In TensorFlow, convolutional layers expect the input to be tensors with the following format: (**rows, height, width, channel**). Depending on the dataset, you may have to reshape the images to conform to this requirement. TensorFlow provides a function for this, shown in the following code snippet:\n",
    "\n",
    "```Python\n",
    "features_train.reshape(60000, 28, 28, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "\n",
    "Another frequent layer in a CNN's architecture is the pooling layer. We have seen previously that the convolutional layer reduces the size of the image if no padding is added. Is this behavior expected? Why don't we keep the exact same size as for the input image? In general, with CNNs, we tend to reduce the size of the feature maps as we progress through different layers. The main reason for this is that we want to have more and more specific pattern detectors closer to the end of the network.\n",
    "\n",
    "Closer to the beginning of the network, a CNN will tend to have more generic filters, such as vertical or horizontal line detectors, but as it goes deeper, we would, for example, have filters that can detect a dog's tail or a cat's whiskers if we were training a CNN to recognize cats versus dogs, or the texture of objects if we were classifying images of fruits. Also, having smaller feature maps reduces the risk of false patterns being detected.\n",
    "\n",
    "By increasing the stride, we can further reduce the size of the output feature map. But there is another way to do this: adding a pooling layer after a convolutional layer. A pooling layer is a matrix of a given size and will apply an aggregation function to each area of the feature map. The most frequent aggregation method is finding the maximum value of a group of pixels:\n",
    "\n",
    "![Figure 6.23](img/fig6_23.jpg)\n",
    "\n",
    "In the preceding example, we use a max pooling of size $(2, 2)$ and `stride=2`. We look at the top-left corner of the feature map and find the maximum value among the pixels 6, 8, 1, and 2 and get the result, 8. Then we slide the max pooling by a stride of 2 and perform the same operation on the pixels 6, 1, 7, and 4. We repeat the same operation on the bottom groups and get a new feature map of size $(2,2)$.\n",
    "\n",
    "In TensorFlow, we can use the `MaxPool2D()` class to declare a max-pooling layer:\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras import layers\n",
    "layers.MaxPool2D(pool_size=(2, 2), strides=2)\n",
    "```\n",
    "\n",
    "  > You can read more about this Conv2D class on TensorFlow's website at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "\n",
    "As you saw earlier, you can define your own custom CNN architecture by specifying the type and number of hidden layers, the activation functions to be used, and so on. But this may be a bit daunting for beginners. How do we know how many filters need to be added at each layer or what the right stride will be? We will have to try multiple combinations and see which ones work.\n",
    "\n",
    "Luckily, a lot of researchers in deep learning have already done such exploratory work and have published the architecture they designed. Currently, the most famous ones are these:\n",
    "\n",
    "  * AlexNet\n",
    "  * VGG\n",
    "  * ResNet\n",
    "  * Inception\n",
    "  \n",
    "  > You can read more about the different CNN architectures implemented on TensorFlow at [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "\n",
    "In the last section, we learned how we can use CNNs for computer vision tasks such as classifying images. With deep learning, computers are now capable of achieving and sometimes surpassing human performance. Another field that is attracting a lot of interest from researchers is natural language processing. This is a field where RNNs excel.\n",
    "\n",
    "In the last few years, we have seen a lot of different applications of RNN technology, such as speech recognition, chatbots, and text translation applications. But RNNs are also quite performant in predicting time series patterns, something that's used for forecasting stock markets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Layers\n",
    "\n",
    "The common point with all the applications mentioned earlier is that the inputs are sequential. There is a time component with the input. For instance, a sentence is a sequence of words, and the order of words matters; stock market data consists of a sequence of dates with corresponding stock prices.\n",
    "\n",
    "To accommodate such input, we need neural networks to be able to handle sequences of inputs and be able to maintain an understanding of the relationships between them. One way to do this is to create memory where the network can take into account previous inputs. This is exactly how a basic RNN works:\n",
    "\n",
    "![Figure 6.24](img/fig6_24.jpg)\n",
    "\n",
    "In the preceding figure, we can see a neural network that takes an input called $X_t$ and performs some transformations and gives the output results, $\\hat{y_t}$. Nothing new so far.\n",
    "\n",
    "But you may have noticed that there is an additional output called $H_{t-1}$ that is an output but also an input to the neural network. This is how RNN simulates memory – by considering its previous results and taking them in as an additional input. Therefore, the result $\\hat{y_t}$ will depend on the input $x_t$ but also $H_{t-1}$. Now, we can represent a sequence of four inputs that get fed into the same neural network:\n",
    "\n",
    "![Figure 6.25](img/fig6_25.jpg)\n",
    "\n",
    "We can see the neural network is taking an input ($x$) and generating an output ($y$) at each time step ($t, t+1, \\dots, t+3$) but also another output ($h$), which is feeding the next iteration.\n",
    "\n",
    "  > The preceding figure may be a bit misleading – there is actually only one RNN here (all the RNN boxes in the middle form one neural network), but it is easier to see how the sequencing works in this format.\n",
    "  \n",
    "An RNN cell looks like this on the inside:\n",
    "\n",
    "![Figure 6.26](img/fig6_26.jpg)\n",
    "\n",
    "It is very similar to a simple neuron, but it takes more inputs and uses `tanh` as the activation function.\n",
    "\n",
    "  > You can use any activation function in an RNN cell. The default value in TensorFlow is `tanh`.\n",
    "\n",
    "This is the basic logic of RNNs. In TensorFlow, we can instantiate an RNN layer with `layers.SimpleRNN`:\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras import layers\n",
    "layers.SimpleRNN(4, activation='tanh')\n",
    "```\n",
    "\n",
    "In the code snippet, we created an RNN layer with $4$ outputs and the `tanh` activation function (which is the most widely used activation function for RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GRU Layer\n",
    "\n",
    "One drawback with the previous type of layer is that the final output takes into consideration all the previous outputs. If you have a sequence of 1,000 input units, the final output, y, is influenced by every single previous result. If this sequence was composed of 1,000 words and we were trying to predict the next word, it would really be overkill to have to memorize all of the 1,000 words before making a prediction. Probably, you only need to look at the previous 100 words from the final output.\n",
    "\n",
    "This is exactly what **Gated Recurrent Unit (GRU)** cells are for. Let's look at what is inside them:\n",
    "\n",
    "![Figure 6.27](img/fig6_27.jpg)\n",
    "\n",
    "Compared to a simple RNN cell, a GRU cell has a few more elements:\n",
    "\n",
    "  * A second activation function, which is `sigmoid`\n",
    "  * A multiplier operation performed before generating the outputs $(y_t)$ and $H_t$\n",
    "  \n",
    "The usual path with `tanh` is still responsible for making a prediction, but this time we will call it the \"candidate.\" The sigmoid path acts as an \"update\" gate. This will tell the GRU cell whether it needs to discard the use of this candidate or not. Remember that the output ranges between 0 and 1. If close to 0, the update gate (that is, the sigmoid path) will say we should not consider this candidate.\n",
    "\n",
    "On the other hand, if it is closer to 1, we should definitely use the result of this candidate.\n",
    "\n",
    "Remember that the output $H_t$ is related to $H_{t-1}$, which is related to $H_{t-2}$, and so on. So, this update gate will also define how much \"memory\" we should keep. It tends to prioritize previous outputs closer to the current one.\n",
    "\n",
    "This is the basic logic of GRU (note that the GRU cell has one more component, the reset gate, but for the purpose of simplicity, we will not look at it). In TensorFlow, we can instantiate such a layer with `layers.GRU`:\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras import layers\n",
    "layers.GRU(4, activation='tanh', recurrent_activation='sigmoid')\n",
    "```\n",
    "\n",
    "In the code snippet, we have created a GRU layer with 4 output units and the `tanh` activation function for the candidate prediction and sigmoid for the update gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LSTM Layer\n",
    "\n",
    "There is another very popular type of cell for RNN architecture called the LSTM cell. LSTM stands for **Long Short-Term Memory**. LSTM came before GRU, but the latter is much simpler, and this is the reason why we presented it first. Here is what is under the hood of LSTM:\n",
    "\n",
    "![Figure 6.28](img/fig6_28.jpg)\n",
    "\n",
    "At first, this looks very complicated. It is composed of several elements:\n",
    "\n",
    "  * `Cell state`: This is the concatenation of all the previous outputs. It is the \"memory\" of the LSTM cell.\n",
    "  * `Forget gate`: This is responsible for defining whether we should keep or forget a given memory.\n",
    "  * `Input gate`: This is responsible for defining whether the new memory candidate needs to be updated or not. This new memory candidate is then added to the previous memory.\n",
    "  * `Output gate`: This is responsible for making the prediction based on the previous output $(H_{t-1}$), the current input $(x_t)$, and the memory.\n",
    "  \n",
    "An LSTM cell can consider previous results but also past memory, and this is the reason why it is so powerful.\n",
    "\n",
    "In TensorFlow, we can instantiate such a layer with `layers.SimpleRNN`:\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras import layers\n",
    "layers.LSTM(4, activation='tanh', recurrent_activation='sigmoid')\n",
    "```\n",
    "\n",
    "In the code snippet, we have created an LSTM layer with 4 output units and the `tanh` activation function for the candidate prediction and sigmoid for the update gate.\n",
    "\n",
    "  > You can read more about SimpleRNN implementation in TensorFlow here: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware for Deep Learning\n",
    "\n",
    "As you may have noticed, training deep learning models takes longer than traditional machine learning algorithms. This is due to the number of calculations required for the forward pass and backpropagation. In this book, we trained very simple models with just a few layers. But there are architectures with hundreds of layers, and some with even more than that. That kind of network can take days or even weeks to train.\n",
    "\n",
    "To speed up the training process, it is recommended to use a specific piece of hardware called a GPU. GPUs specialize in performing mathematical operations and therefore are perfect for deep learning. Compared to a **Central Processing Unit (CPU)**, a GPU can be up to 10X faster at training a deep learning model. You can personally buy a GPU and set up your own deep learning computer. You just need to get one that is CUDA-compliant (currently only NVIDIA GPUs are).\n",
    "\n",
    "Another possibility is to use cloud providers such as AWS or Google Cloud Platform and train your models in the cloud. You will pay only for what you use and can switch them off as soon as you are done. The benefit is that you can scale the configuration up or down depending on the needs of your projects – but be mindful of the cost. You will be charged for the time your instance is up even if you are not training a model. So, don't forget to switch things off if you're not using them.\n",
    "\n",
    "Finally, Google recently released some new hardware dedicated to deep learning: **Tensor Processing Unit (TPUs)**. They are much faster than GPUs, but they are quite costly. Currently, only Google Cloud Platform provides such hardware in their cloud instances.\n",
    "\n",
    "## Challenges and Future Trends\n",
    "\n",
    "As with any new technology, deep learning comes with challenges. One of them is the big barrier to entry. To become a deep learning practitioner, you used to have to know all the mathematical theory behind deep learning very well and be a confirmed programmer. On top of this, you had to learn the specifics of the deep learning framework you chose to use (be it TensorFlow, PyTorch, Caffe, or anything else). For a while, deep learning couldn't reach a broad audience and was mainly limited to researchers. This situation has changed, though it is not perfect. For instance, TensorFlow now comes with a higher-level API called Keras (this is the one you saw in this chapter) that is much easier to use than the core API. Hopefully, this trend will keep going and make deep learning frameworks more accessible to anyone interested in this field.\n",
    "\n",
    "The second challenge was that deep learning models require a lot of computation power, as mentioned in the previous section. This was again a major blocker for anyone who wanted to have a go at it. Even though the cost of GPUs has gone down, deep learning still requires some upfront investment. Luckily for us, there is now a free option to train deep learning models with GPUs: Google Colab. It is an initiative from Google to promote research by providing temporary cloud computing for free. The only thing you need is a Google account. Once signed up, you can create Notebooks (similar to Jupyter Notebooks) and choose a kernel to be run on a CPU, GPU (limited to 10 hours per day), or even a TPU (limited to ½ hour per day). So, before investing in purchasing or renting out GPU, you can first practice with Google Colab.\n",
    "\n",
    "  > You can find more information about Google Colab at [https://colab.research.google.com/](https://colab.research.google.com/).\n",
    "\n",
    "More advanced deep learning models can be very deep and require weeks of training. So, it is hard for basic practitioners to use such architecture. But thankfully, a lot of researchers have embraced the open source movement and have shared not only the architectures they have designed but also the weights of the networks. This means you can now access state-of-the-art pre-trained models and fine-tune them to fit your own projects. This is called transfer learning (which is out of the scope of this book). It is very popular in computer vision, where you can find pre-trained models on ImageNet or MS-Coco, for instance, which are large datasets of pictures. Transfer learning is also happening in natural language processing, but it is not as developed as it is for computer vision.\n",
    "\n",
    "  > You can find more information about these datasets at [http://www.image-net.org/ and http://cocodataset.org/](http://www.image-net.org/ and http://cocodataset.org/).\n",
    "  \n",
    "Another very important topic related to deep learning is the increasing need to be able to interpret model results. Soon, these kinds of algorithms may be regulated, and deep learning practitioners will have to be able to explain why a model is making a given decision. Currently, deep learning models are more like black boxes due to the complexity of the networks. There are already some initiatives from researchers to find ways to interpret and understand deep neural networks, such as Zeiler and Fergus, \"Visualizing and Understanding Convolutional Networks\", ECCV 2014. However, more work needs to be done in this field with the democratization of such technologies in our day-to-day lives. For instance, we will need to make sure that these algorithms are not biased and are not making unfair decisions affecting specific groups of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
