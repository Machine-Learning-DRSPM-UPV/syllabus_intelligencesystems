{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence: Clustering\n",
    "\n",
    "This section will introduce you to the fundamentals of clustering, an unsupervised learning approach in contrast with the supervised learning approaches seen in the previous chapters. You will be implementing different types of clustering, including flat clustering with the k-means algorithm and hierarchical clustering with the mean shift algorithm and the agglomerative hierarchical model. You will also learn how to evaluate the performance of your clustering model using intrinsic and extrinsic approaches. By the end, you will be able to analyze data using clustering and apply this skill to solve challenges across a variety of fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Previously, you were introduced to decision trees and their applications in classification. You were also introduced to regression. Both regression and classification are part of the supervised learning approach. However, in this section, we will be looking at the unsupervised learning approach; we will be dealing with datasets that don't have any labels (outputs). It is up to the machines to tell us what the labels will be based on a set of parameters that we define. We will be performing unsupervised learning by using clustering algorithms.\n",
    "\n",
    "We will use clustering to analyze data to find certain patterns and create groups. Apart from that, clustering can be used for many purposes:\n",
    "\n",
    "  * Market segmentation detects the best stocks in the market you should be focusing on.\n",
    "  * Customer segmentation detects customer cohorts using their consumption patterns to recommend products better.\n",
    "  * In computer vision, image segmentation is performed using clustering. Using this, we can find different objects in an image.\n",
    "  * Clustering can be also be combined with classification to generate a compact representation of multiple features (inputs), which can then be fed to a classifier.\n",
    "  * Clustering can also filter data points by detecting outliers.\n",
    "  \n",
    "Regardless of whether we are applying clustering to genetics, videos, images, or social networks, if we analyze data using clustering, we may find similarities between data points that are worth treating uniformly.\n",
    "\n",
    "For instance, consider a store manager, who is responsible for ensuring the profitability of their store. The products in the store are divided into different categories, and there are different customers who prefer different items. Each customer has their own preferences, but they have some similarities between them. You might have a customer who is interested in bio products, who tends to choose organic products, which are also of interest to a vegetarian customer. Even if they are different, they have similarities in their preferences or patterns as they both tend to buy organic vegetables. This can be treated as an example of clustering.\n",
    "\n",
    "In the section *\"An Introduction to Classification\"*, you learned about classification, which is a part of the supervised learning approach. In a classification problem, we use labels to train a model in order to be able to classify data points. With clustering, as we do not have labels for our features, we need to let the model figure out the clusters to which these features belong. This is usually based on the distance between each data point.\n",
    "\n",
    "In this chapter, you will learn about the k-means algorithm, which is the most widely used algorithm for clustering, but first, we need to define what the clustering problem is.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Clustering Problem\n",
    "\n",
    "We shall define the clustering problem so that we will be able to find similarities between our data points. For instance, suppose we have a dataset that consists of points. Clustering helps us understand this structure by describing how these points are distributed.\n",
    "\n",
    "Let's look at an example of data points in a two-dimensional space in Figure 5.1:\n",
    "\n",
    "![Figure 5.1](img/fig5_01.jpg)\n",
    "\n",
    "Now, have a look, at Figure 5.2. It is evident that there are three clusters:\n",
    "\n",
    "![Figure 5.2](img/fig5_02.jpg)\n",
    "\n",
    "The three clusters were easy to detect because the points are close to one another. Here, you can see that clustering determines the data points that are close to each other. You may have also noticed that the data points $M_1$, $O_1$, and $N_1$ do not belong to any cluster; these are the **outlier points**. The clustering algorithm you build should be prepared to treat these outlier points properly, without moving them into a cluster.\n",
    "\n",
    "While it is easy to recognize clusters in a two-dimensional space, we normally have multidimensional data points, which is where we have more than two features. Therefore, it is important to know which data points are close to one other. Also, it is important to define the distance metrics that detect whether data points are close to each other. One well-known distance metric is Euclidean distance, which we learned about in *Introduction to Artificial Intelligence*. In mathematics, we often use Euclidean distance to measure the distance between two points. Therefore, Euclidean distance is an intuitive choice when it comes to clustering algorithms so that we can determine the proximity of data points when locating clusters.\n",
    "\n",
    "However, there is one drawback to most distance metrics, including Euclidean distance: the more we increase the dimensions, the more uniform these distances will become compared to each other. When we only have a few dimensions or features, it is easy to see which point is the closest to another one. However, when we add more features, the relevant features get embedded with all the other data and it becomes very hard to distinguish the relevant features from the others as they act as noise for our model. Therefore, getting rid of these noisy features may greatly increase the accuracy of our clustering model.\n",
    "\n",
    "  > Noise in a dataset can be irrelevant information or randomness that is unwanted.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Approaches\n",
    "\n",
    "There are two types of clustering:\n",
    "\n",
    "  * Flat\n",
    "  * Hierarchical\n",
    "  \n",
    "In flat clustering, we specify the number of clusters we would like the machine to find. One example of flat clustering is the k-means algorithm, where k specifies the number of clusters we would like the algorithm to use.\n",
    "\n",
    "In hierarchical clustering, however, the machine learning algorithm itself finds out the number of clusters that are needed.\n",
    "\n",
    "Hierarchical clustering also has two approaches:\n",
    "\n",
    "  * **Agglomerative or bottom-up hierarchical clustering** treats each point as a cluster to begin with. Then, the closest clusters are grouped together. The grouping is repeated until we reach a single cluster with every data point.\n",
    "  * **Divisive or top-down hierarchical clustering** treats data points as if they were all in one single cluster at the start. Then the cluster is divided into smaller clusters by choosing the furthest data points. The splitting is repeated until each data point becomes its own cluster.\n",
    "  \n",
    "Figure 5.3 gives you a much more accurate description of these two clustering approaches.\n",
    "\n",
    "![Figure 5.3](img/fig5_03.jpg)\n",
    "\n",
    "Now that we are familiar with the different clustering approaches, let's take a look at the different clustering algorithms supported by scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Algorithms Supported by scikit-learn\n",
    "\n",
    "In this section, we will learn about two clustering algorithms supported by scikit-learn:\n",
    "\n",
    "  * The k-means algorithm\n",
    "  * The mean shift algorithm\n",
    "  \n",
    "**K-means** is an example of flat clustering, where we must specify the number of clusters in advance. k-means is a general-purpose clustering algorithm that performs well if the number of clusters is not too high and the size of the clusters is uniform.\n",
    "\n",
    "**Mean shift** is an example of hierarchical clustering, where the clustering algorithm determines the number of clusters. Mean shift is used when we do not know the number of clusters in advance. In contrast with k-means, mean shift supports use cases where there may be many clusters present, even if the size of the clusters greatly varies.\n",
    "\n",
    "Scikit-learn contains many other algorithms, but we will be focusing on the k-means and mean shift algorithms.\n",
    "\n",
    "  > For a complete description of clustering algorithms, including performance comparisons, visit the clustering page of scikit-learn at [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The K-Means Algorithm\n",
    "\n",
    "The k-means algorithm is a flat clustering algorithm, as mentioned previously. It works as follows:\n",
    "\n",
    "  * Set the value of $k$.\n",
    "  * Choose $k$ data points from the dataset that are the initial centers of the individual clusters.\n",
    "  * Calculate the distance from each data point to the chosen center points and group each point in the cluster whose initial center is the closest to the data point.\n",
    "  * Once all the points are in one of the k clusters, calculate the center point of each cluster. This center point does not have to be an existing data point in the dataset; it is simply an average.\n",
    "  * Repeat this process of assigning each data point to the cluster whose center is closest to the data point. Repetition continues until the center points no longer move.\n",
    "  \n",
    "To ensure that the k-means algorithm terminates, we need the following:\n",
    "\n",
    "  * A maximum threshold value at which the algorithm will then terminate\n",
    "  * A maximum number of repetitions of shifting the moving points\n",
    "  \n",
    "Due to the nature of the k-means algorithm, it will have a hard time dealing with clusters that greatly vary in size.\n",
    "\n",
    "The k-means algorithm has many use cases that are part of our everyday lives, such as:\n",
    "\n",
    "  * **Market segmentation**: Companies gather all sorts of data on their customers. Performing k-means clustering analysis on their customers will reveal customer segments (clusters) with defined characteristics. Customers belonging to the same segment can be seen as having similar patterns or preferences.\n",
    "  * **Tagging of content**: Any content (videos, books, documents, movies, or photos) can be assigned tags in order to group together similar content or themes. These tags are the result of clustering.\n",
    "  * **Detection of fraud and criminal activities**: Fraudsters often leave clues in the form of unusual behaviors compared to other customers. For instance, in the car insurance industry, a normal customer will make a claim for a damaged car arising from an incident, whereas fraudsters will make claims for deliberate damage. Clustering can help detect whether the damage has arisen from a real accident or from a fake accident.\n",
    "  \n",
    "**Go to Exercise 5.01**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Parameterization of the K-Means Algorithm in scikit-learn\n",
    "\n",
    "Like the classification and regression models, the k-means algorithm from scikit-learn can also be parameterized. The complete list of parameters can be found at [http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
    "\n",
    "Some examples are as follows:\n",
    "\n",
    "  * `n_clusters`: The number of clusters into which the data points are separated. The default value is 8.\n",
    "  * `max_iter`: The maximum number of iterations.\n",
    "  * `tol`: The threshold for checking whether we can terminate the k-means algorithm.\n",
    "  \n",
    "We also used two attributes to retrieve the cluster center points and the clusters themselves:\n",
    "\n",
    "  * `cluster_centers_`: This returns the coordinates of the cluster center points.\n",
    "  * `labels_`: This returns an array of integers representing the number of clusters the data point belongs to. Numbering starts from zero.\n",
    "  \n",
    "**Go to Exercise 5.02**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mean Shift Algorithm\n",
    "\n",
    "Mean shift is a hierarchical clustering algorithm that assigns data points to a cluster by calculating a cluster's center and moving it towards the mode at each iteration. The mode is the area with the most data points. At the first iteration, a random point will be chosen as the cluster's center and then the algorithm will calculate the mean of all nearby data points within a certain radius. The mean will be the new cluster's center. The second iteration will then begin with the calculation of the mean of all nearby data points and setting it as the new cluster's center. At each iteration, the cluster's center will move closer to where most of the data points are. The algorithm will stop when it is not possible for a new cluster's center to contain more data points. When the algorithm stops, each data point will be assigned to a cluster.\n",
    "\n",
    "The mean shift algorithm will also determine the number of clusters needed, in contrast with the k-means algorithm. This is advantageous as we rarely know how many clusters we are looking for.\n",
    "\n",
    "This algorithm also has many use cases. For instance, the Xbox Kinect device detects human body parts using the mean shift algorithm. Each main body part (head, arms, legs, hands, and so on) is a cluster of data points assigned by the mean shift algorithm.\n",
    "\n",
    "**Go to Exercise 5.03**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a simple clustering example that applied the mean shift algorithm. We only illustrated what the algorithm considers when finding clusters.\n",
    "\n",
    "However, there is still one question, and that is what will the value of the radius be?\n",
    "\n",
    "Note that if the radius of 2 was not set, we could simply start either with a huge radius that includes all data points and then reduce the radius, or we could start with a tiny radius, making sure that each data point is in its cluster, and then increase the radius until we get the desired result.\n",
    "\n",
    "In the next section, we will be looking at the mean shift algorithm but using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mean Shift Algorithm in scikit-learn\n",
    "\n",
    "Let's use the same data points we used with the k-means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_points = np.array([[1, 1], [1, 1.5], [2, 2],\n",
    "                        [8, 1], [8, 0], [8.5, 1],\n",
    "                        [6, 1], [1, 10], [1.5, 10],\n",
    "                        [1.5, 9.5], [10, 10], [1.5, 8.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax of the mean shift clustering algorithm is like the syntax for the k-means clustering algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeanShift()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "mean_shift_model = MeanShift()\n",
    "mean_shift_model.fit(data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the clustering is done, we can access the center point of each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.375     ,  9.5       ],\n",
       "       [ 8.16666667,  0.66666667],\n",
       "       [ 1.33333333,  1.5       ],\n",
       "       [10.        , 10.        ],\n",
       "       [ 6.        ,  1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_shift_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean shift model found five clusters with the centers shown in the preceding code.\n",
    "\n",
    "Like k-means, we can also get the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 1, 1, 4, 0, 0, 0, 3, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_shift_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output array shows which data point belongs to which cluster. This is all we need to plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(mean_shift_model.cluster_centers_[:, 0],\n",
    "           mean_shift_model.cluster_centers_[:, 1])\n",
    "\n",
    "for i in range(len(data_points)):\n",
    "    plt.plot(data_points[i][0], data_points[i][1],\n",
    "            ['k+', 'kx', 'kv', 'k_', 'k1'][mean_shift_model.labels_[i]])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code snippet, we made a plot of the data points and the centers of the five clusters. Each data point belonging to the same cluster will have the same marker. The cluster centers are marked as a dot.\n",
    "\n",
    "We can see that three clusters contain more than a single dot (the top left, the bottom left, and the bottom right). The two single data points that are also their own cluster can be seen as outliers, as mentioned previously, as they are too far from the other clusters to be part of any of them.\n",
    "\n",
    "Now that we have learned about the mean shift algorithm, we can have look at hierarchical clustering, and more specifically at agglomerative hierarchical clustering (the bottom-up approach)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering algorithms fall into two categories:\n",
    "\n",
    "  * Agglomerative (or bottom-up) hierarchical clustering\n",
    "  * Divisive (or top-down) hierarchical clustering\n",
    "  \n",
    "We will only talk about agglomerative hierarchical clustering in this chapter, as it is the most widely used and most efficient of the two approaches.\n",
    "\n",
    "Agglomerative hierarchical clustering treats each data point as a single cluster in the beginning and then successively merges (or agglomerates) the closest clusters together in pairs. In order to find the closest data clusters, agglomerative hierarchical clustering uses a heuristic such as the Euclidean or Manhattan distance to define the distance between data points. A linkage function will also be required to aggregate the distance between data points in clusters in order to define a unique value of the closeness of clusters.\n",
    "\n",
    "Examples of linkage functions include single linkage (simple distance), average linkage (average distance), maximum linkage (maximum distance), and Ward linkage (square difference). The pairs of clusters with the smallest value of linkage will be grouped together. The grouping is repeated until we reach a single cluster containing every data point. In the end, this algorithm terminates when there is only a single cluster left.\n",
    "\n",
    "In order to visually represent the hierarchy of clusters, a dendrogram can be used. A dendrogram is a tree where the leaves at the bottom represent data points. Each intersection between two leaves is the grouping of these two leaves. The root (top) represents a unique cluster that contains all the data points. Have a look at Figure 5.10, which represents a dendrogram.\n",
    "\n",
    "![Figuree 5.10](img/fig5_10.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Hierarchical Clustering in scikit-learn\n",
    "\n",
    "Have a look at the following example, where we use the same data points as we used with the k-means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_points = np.array([[1, 1], [1, 1.5], [2, 2],\n",
    "                        [8, 1], [8, 0], [8.5, 1],\n",
    "                        [6, 1], [1, 10], [1.5, 10],\n",
    "                        [1.5, 9.5], [10, 10], [1.5, 8.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot a dendrogram, we need to first import the `scipy` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can plot a dendrogram using SciPy with the `ward` linkage function, as it is the most commonly used linkage function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dendrogram = sch.dendrogram(sch.linkage(data_points, method='ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dendrogram, we can generally guess what will be a good number of clusters by simply drawing a horizontal line as shown in Figure 5.12, in the area with the highest vertical distance, and counting the number of intersections. In this case, it should be two clusters, but we will go to the next biggest area as two is too small a number.\n",
    "\n",
    "![Figure 5.12](img/fig5_12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y axis represents the measure of closeness, and the x axis represents the index of each data point. So our first three data points (0,1,2) are parts of the same cluster, then another cluster is made of the next four points (3,4,5,6), data point 10 is a cluster on its own, and the remaining data points (7,8,9,11) form the last cluster.\n",
    "\n",
    "The syntax of the agglomerative hierarchical clustering algorithm is similar to the k-means clustering algorithm except that we need to specify the number type of `affinity` (here, we choose the Euclidean distance) and the linkage (here, we choose the ward linkage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(n_clusters=4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agglomerative_model = AgglomerativeClustering(n_clusters=4,\n",
    "                                             affinity='euclidean',\n",
    "                                             linkage='ward')\n",
    "agglomerative_model.fit(data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to k-means, we can also get the labels as shown in the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 3, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agglomerative_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output array shows which data point belongs to which cluster. This is all we need to plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(data_points)):\n",
    "    plt.plot(data_points[i][0], data_points[i][1],\n",
    "            ['k+', 'kx', 'kv', 'k_'][agglomerative_model.labels_[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, in contrast with the result from the mean shift method, agglomerative clustering was able to properly group the data point at (6,1) with the bottom-right cluster instead of having his own cluster. In situations like this one, where we have a very small amount of data, agglomerative hierarchical clustering and mean shift will work better than k-means. However, they have very expensive computational time requirements, which will make them struggle on very large datasets. However, k-means is very fast and is a better choice for very large datasets.\n",
    "\n",
    "Now that we have learned about a few different clustering algorithms, we need to start evaluating these models and comparing them in order to choose the best model for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Performance Evaluation\n",
    "\n",
    "Unlike supervised learning, where we always have the labels to evaluate our predictions with, unsupervised learning is a bit more complex as we do not usually have labels. In order to evaluate a clustering model, two approaches can be taken depending on whether the label data is available or not:\n",
    "\n",
    "  * The first approach is the extrinsic method, which requires the existence of label data. This means that in absence of label data, human intervention is required in order to label the data or at least a subset of it.\n",
    "  * The other approach is the intrinsic approach. In general, the extrinsic approach tries to assign a score to clustering, given the label data, whereas the intrinsic approach evaluates clustering by examining how well the clusters are separated and how compact they are.\n",
    "  \n",
    "  > We will skip the mathematical explanations as they are quite complicated. You can find more mathematical details on the sklearn website at [this URL](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)\n",
    "\n",
    "We will begin with the extrinsic approach (as it is the most widely used method) and define the following scores using sklearn on our k-means example:\n",
    "\n",
    "  * The adjusted Rand index\n",
    "  * The adjusted mutual information\n",
    "  * The homogeneity\n",
    "  * The completeness\n",
    "  * The V-Measure\n",
    "  * The Fowlkes-Mallows score\n",
    "  * The contingency matrix\n",
    "  \n",
    "Let's have a look at an example in which we first need to import the `metrics` module from `sklearn.cluster`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be reusing the code from our k-means example in Exercise 5.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data_points = np.array([[1, 1], [1, 1.5], [2, 2],\n",
    "                        [8, 1], [8, 0], [8.5, 1],\n",
    "                        [6, 1], [1, 10], [1.5, 10],\n",
    "                        [1.5, 9.5], [10, 10], [1.5, 8.5]])\n",
    "k_means_model = KMeans(n_clusters=3, random_state=8)\n",
    "k_means_model.fit(data_points)\n",
    "k_means_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the true labels of this dataset, as shown in the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = np.array([0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Adjusted Rand Index\n",
    "\n",
    "The adjusted Rand index is a function that measures the similarity between the cluster predictions and the labels while ignoring permutations. The adjusted Rand index works quite well when the labels are large equal-sized clusters.\n",
    "\n",
    "The adjusted Rand index has a range between $[-1.1]$, where negative values are not desirable. A negative score means that our model is performing worse than if we were to randomly assign labels. If we were to randomly assign them, our score would be close to 0. However, the closer we are to 1, the better our clustering model is at predicting the right label.\n",
    "\n",
    "With `sklearn`, we can easily compute the adjusted Rand index by using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8422939068100358"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_rand_score(data_labels, k_means_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the adjusted Rand index indicates that our k-means model is not far from our true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Adjusted Mutual Information\n",
    "\n",
    "The adjusted mutual information is a function that measures the entropy between the cluster predictions and the labels while ignoring permutations.\n",
    "\n",
    "The adjusted mutual information has no defined range, but negative values are considered bad. The closer we are to 1, the better our clustering model is at predicting the right label.\n",
    "\n",
    "With `sklearn`, we can easily compute it by using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8769185235006342"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_mutual_info_score(data_labels, k_means_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the adjusted mutual information indicates that our k-means model is quite good and not far from our true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The V-Measure, Homogeneity, and Completeness\n",
    "\n",
    "The V-Measure is defined as the harmonic mean of homogeneity and completeness. The harmonic mean is a type of average (other types are the arithmetic mean and the geometric mean) using reciprocals (a reciprocal is the inverse of a number. For example the reciprocal of 2 is $\\frac{1}{2}$, and the reciprocal of 3 is $\\frac{1}{3}$).\n",
    "\n",
    "The formula of the harmonic mean is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{n}{\\sum{} \\frac{1}{x_i}}\n",
    "$$\n",
    "\n",
    "$n$ is the number of values and $x_i$ 4 is the value of each point.\n",
    "\n",
    "In order to calculate the V-Measure, we first need to define homogeneity and completeness.\n",
    "\n",
    "Perfect homogeneity refers to a situation where each cluster has data points belonging to the same label. The homogeneity score will reflect how well each of our clusters is grouping data from the same label.\n",
    "\n",
    "Perfect completeness refers to the situation where all data points belonging to the same label are clustered into the same cluster. The homogeneity score will reflect how well, for each of our labels, its data points are all grouped inside the same cluster.\n",
    "\n",
    "Hence, the formula of V-Measure is as follows:\n",
    "\n",
    "$$\n",
    "v = \\frac{\\left(1 + \\beta \\right) \\times homogeneity \\times completeness}{\\left( \\beta \\times homogeneity \\times completeness \\right)}\n",
    "$$\n",
    "\n",
    "$\\beta$ has a default value of 1, but it can be changed to further emphasize either homogeneity or completeness.\n",
    "\n",
    "These three scores have a range between $[0,1]$, with 0 being the worst possible score and 1 being the perfect score.\n",
    "\n",
    "With `sklearn`, we can easily compute these three scores by using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8378758055108827"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.homogeneity_score(data_labels, k_means_model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.completeness_score(data_labels, k_means_model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9117871871412709"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.v_measure_score(data_labels, k_means_model.labels_, beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * The homogeneity score indicates that our k-means model has clusters containing different labels.\n",
    "  * The completeness score indicates that our k-means model has successfully put every data point of each label inside the same cluster.\n",
    "  * The V-Measure indicates that our k-means model, while not being perfect, has a good score in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fowlkes-Mallows Score\n",
    "\n",
    "The Fowlkes-Mallows score is a metric measuring the similarity within a label cluster and the prediction of the cluster, and this is defined as the geometric mean of the precision and recall.\n",
    "\n",
    "The formula of the Fowlkes-Mallows score is as follows:\n",
    "\n",
    "$$\n",
    "FMI = \\frac{TP}{\\sqrt{\\left(TP + FP \\right) \\left( TP + FN \\right)}}\n",
    "$$\n",
    "\n",
    "Let's break this down:\n",
    "\n",
    "  * True positive (or TP): Are all the observations where the predictions are in the same cluster as the label cluster\n",
    "  * False positive (or FP): Are all the observations where the predictions are in the same cluster but not the same as the label cluster\n",
    "  * False negative (or FN): Are all the observations where the predictions are not in the same cluster but are in the same label cluster\n",
    " \n",
    "The Fowlkes-Mallows score has a range between $[0, 1]$, with 0 being the worst possible score and 1 being the perfect score.\n",
    "\n",
    "With sklearn, we can easily compute it by using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8885233166386386"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.fowlkes_mallows_score(data_labels, k_means_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the Fowlkes-Mallows score indicates that our k-means model is quite good and not far from our true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Contingency Matrix\n",
    "\n",
    "The contingency matrix is not a score, but it reports the intersection cardinality for every true/predicted cluster pair and the required label data. It is very similar to the Confusion Matrix. The matrix must be the same for the label and cluster name, so we need to be careful to give our cluster the same name as our label, which was not the case with the previously seen scores.\n",
    "\n",
    "We will modify our labels from this:\n",
    "\n",
    "```Python\n",
    "data_labels = np.array([0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 3, 1])\n",
    "```\n",
    "\n",
    "to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = np.array([2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 3, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, with `sklearn`, we can easily compute the contingency matrix by using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 4, 0, 0],\n",
       "       [4, 0, 0, 1],\n",
       "       [0, 0, 3, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "contingency_matrix(k_means_model.labels_, data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row of the `contingency_matrix` output indicates that there are 4 data points whose true cluster is the first cluster (0). The second row indicates that there are also four data points whose true cluster is the second cluster (1); however, an extra 1 was incorrectly predicted in this cluster, but it belongs to the fourth cluster (3). The third row indicates that there are three data points whose true cluster is the third cluster (2).\n",
    "\n",
    "We will now look at the intrinsic approach, which is required when we do not have the label. We will define the following scores using sklearn on our k-means example:\n",
    "\n",
    "  * The Silhouette Coefficient\n",
    "  * The Calinski-Harabasz index\n",
    "  * The Davies-Bouldin index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Silhouette Coefficient\n",
    "\n",
    "The Silhouette Coefficient is an example of an intrinsic evaluation. It measures the similarity between a data point and its cluster when compared to other clusters.\n",
    "\n",
    "It comprises two scores:\n",
    "\n",
    "  * $a$: The average distance between a data point and all other data points in the same cluster.\n",
    "  * $b$: The average distance between a data point and all the data points in the nearest cluster.\n",
    "  \n",
    "The Silhouette Coefficient formula is:\n",
    "\n",
    "$$\n",
    "S = \\frac{b-a}{\\max{\\left( a, b \\right)}}\n",
    "$$\n",
    "\n",
    "The Silhouette Coefficient has a range between $[-1,1]$, with -1 meaning an incorrect clustering. A score close to zero indicates that our clusters are overlapping. A score close to 1 indicates that all the data points are assigned to the appropriate clusters.\n",
    "\n",
    "Then, with sklearn, we can easily compute the silhouette coefficient by using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6753568188872228"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.silhouette_score(data_points, k_means_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the Silhouette Coefficient indicates that our k-means model has some overlapping clusters, and some improvements can be made by separating some of the data points from one of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Calinski-Harabasz Index\n",
    "\n",
    "The Calinski-Harabasz index measures how the data points inside each cluster are spread. It is defined as the ratio of the variance between clusters and the variance inside each cluster. The Calinski-Harabasz index doesn't have a range and starts from 0. The higher the score is, the denser our clusters are. A dense cluster is an indication of a well-defined cluster.\n",
    "\n",
    "With sklearn, we can easily compute it by using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.52509172315154"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.calinski_harabasz_score(data_points, k_means_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the Calinski-Harabasz index indicates that our k-means model clusters are quite spread out and suggests that we might have overlapping clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Davies-Bouldin Index\n",
    "\n",
    "The Davies-Bouldin index measures the average similarity between clusters. The similarity is a ratio of the distance between a cluster and its closest cluster and the average distance between each data point of a cluster and it's cluster's center. The Davies-Bouldin index doesn't have a range and starts from 0. The closer the score is to 0 the better; it means the clusters are well separated, which is an indication of a good cluster.\n",
    "\n",
    "With sklearn, we can easily compute the Davis-Bouldin index by using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.404206621415983"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.davies_bouldin_score(data_points, k_means_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the Calinski-Harabasz score indicates that our k-means model has some overlapping clusters and an improvement could be made by better separating some of the data points in one of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
